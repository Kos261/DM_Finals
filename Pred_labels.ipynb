{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a802eae",
   "metadata": {},
   "source": [
    "Data for this project consists of two tables in a tab-separated columns format. Each row in those files corresponds to an abstract of a scientific article from ACM Digital Library, which was assigned to one or more topics from the  ACM Computing Classification System (the old one from 1998).\n",
    "\n",
    "There are two data sets for this task, the training and the testing sample, respectively. They are text (TSV - tab separated values) files compressed using 7zip.\n",
    "\n",
    "The training data (DM2023_training_docs_and_labels.tsv) has three columns: the first one is an identifier of a document, the second one stores the text of the abstract, and the third one contains a list of comma-separated topic labels.\n",
    "\n",
    "The test data (DM2023_test_docs.tsv) has a similar format, but the labels in the third column are missing.\n",
    "\n",
    "**The task and the format of submissions:** the task for you is to predict the labels of documents from the test data and submit them to the moodle using the link below. A correctly formatted submission should be a text file with exactly 100000 lines plus the report. Each line should correspond to a document from the test data set (the order matters!) and contain a list of one or more predicted labels, separated by commas. The report can be in the form of R/Python notebook (with code and computation outcomes). Please remember about explanations and visualizations – make this report as interesting for a reader as you can.\n",
    "\n",
    "You may make several submissions (up to 20), so please remember to clearly mark the final version of your answer in case there is more than one.\n",
    "\n",
    "Practical note: The submission size in moodle is limited to 512MB. In case your files are larger please use compression (7z,gz, ...) other than Zip. Moodle does not like .zip files. \n",
    "\n",
    "Evaluation: the quality of submissions will be evaluated using a script to compute the average F1-score measure, i.e., for each test document, the F1-score between the predicted and true labels will be computed, and the values obtained for all test cases will be averaged.\n",
    "\n",
    "The deadline for sending the reports is Sunday, June 15.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "*** Data links ***: \n",
    "\n",
    "* [Train text & labels](https://drive.google.com/file/d/1xtVxkhUN__lA3m3cWU5Z6UCI7PJuJ1F3/view)\n",
    "\n",
    "* [Only text](https://drive.google.com/file/d/1TM-L7fEYdNHOlCFWHcw8ksMNrkoodMfk/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf63f5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Categorizing documents in **ACM Digital Library** example: \n",
    "\n",
    "(Main Class).(Subclass).(Subsubcategory)\n",
    "\n",
    "**H.3.5**\n",
    "\n",
    "* H. Information Systems\n",
    "    *  H.3 Information Storage and Retrieval\n",
    "        * H.3.5 Online Information Services\n",
    "\n",
    "**D.3.2**\n",
    "\n",
    "* D. Software\n",
    "    * D.3 Programming Languages\n",
    "        * D.3.2 Language Classifications\n",
    "  \n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1dd6ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:16:59.198249Z",
     "start_time": "2025-06-01T14:16:56.074333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/konstanty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique topics: 358\n",
      "First 10 example topics:  ['A.0', 'A.1', 'A.2', 'A.m', 'B.0', 'B.1', 'B.1.0', 'B.1.1', 'B.1.2', 'B.1.3']\n"
     ]
    }
   ],
   "source": [
    "# import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "test_path = os.path.join(notebook_dir, \"data\", \"DM2023_test_docs.tsv\")\n",
    "train_path = os.path.join(notebook_dir, \"data\", \"DM2023_training_docs_and_labels.tsv\")\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "test = pd.read_csv(test_path, \n",
    "                    sep=\"\\t\", \n",
    "                    encoding=\"latin1\", \n",
    "                    header=None,\n",
    "                    names=[\"Textfile\", \"Text\", \"Topics\"])\n",
    "# test = test.drop_duplicates()\n",
    "                    \n",
    "                    \n",
    "train_full = pd.read_csv(train_path, \n",
    "                    sep=\"\\t\", \n",
    "                    encoding=\"latin1\", \n",
    "                    header=None,\n",
    "                    names=[\"Textfile\", \"Text\", \"Topics\"])\n",
    "\n",
    "\n",
    "def flatten_if_single(x):\n",
    "    \"\"\"Jeśli x jest listą długości 1 – zwróć jej pierwszy element.\"\"\"\n",
    "    if isinstance(x, list) and len(x) == 1:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "# Separating topics\n",
    "train_full[\"Topics\"] = (\n",
    "    train_full[\"Topics\"]\n",
    "    .apply(flatten_if_single)        \n",
    "    .str.split(r\"\\s*,\\s*\")         \n",
    ")\n",
    "\n",
    "# train[\"Topics\"] = train[\"Topics\"].str.split(\",\")\n",
    "\n",
    "unique_labels = set(label for sublist in train_full[\"Topics\"] for label in sublist)\n",
    "\n",
    "print(f\"Number of unique topics: {len(unique_labels)}\")\n",
    "print(\"First 10 example topics: \",sorted(list(unique_labels))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7af26a",
   "metadata": {},
   "source": [
    "# Train LDA, MLB (Or load) and topic distribution \n",
    "(Shape of mlb binary matrix should match the number of unique topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a3062a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (80000, 3)  Val: (20000, 3)\n",
      "Vectorizer loaded\n",
      "LDA loaded\n",
      "… training MultiLabelBinarizer\n",
      "Vectorizer vocab size: 50818\n",
      "LDA topics: 200\n",
      "y_train shape: (80000, 358)\n",
      "y_val   shape: (20000, 358)\n",
      "['A.0' 'A.1' 'A.2' 'A.m' 'B.0' 'B.1' 'B.1.0' 'B.1.1' 'B.1.2' 'B.1.3'\n",
      " 'B.1.4' 'B.1.5' 'B.1.m' 'B.2' 'B.2.0' 'B.2.1' 'B.2.2' 'B.2.3' 'B.2.4'\n",
      " 'B.2.m' 'B.3' 'B.3.0' 'B.3.1' 'B.3.2' 'B.3.3' 'B.3.4' 'B.3.m' 'B.4'\n",
      " 'B.4.0' 'B.4.1' 'B.4.2' 'B.4.3' 'B.4.4' 'B.4.5' 'B.4.m' 'B.5' 'B.5.0'\n",
      " 'B.5.1' 'B.5.2' 'B.5.3' 'B.5.m' 'B.6' 'B.6.0' 'B.6.1' 'B.6.2' 'B.6.3'\n",
      " 'B.6.m' 'B.7' 'B.7.0' 'B.7.1' 'B.7.2' 'B.7.3' 'B.7.m' 'B.8' 'B.8.0'\n",
      " 'B.8.1' 'B.8.2' 'B.8.m' 'B.m' 'C.0' 'C.1' 'C.1.0' 'C.1.1' 'C.1.2' 'C.1.3'\n",
      " 'C.1.4' 'C.1.m' 'C.2' 'C.2.0' 'C.2.1' 'C.2.2' 'C.2.3' 'C.2.4' 'C.2.5'\n",
      " 'C.2.6' 'C.2.m' 'C.3' 'C.4' 'C.5' 'C.5.0' 'C.5.1' 'C.5.2' 'C.5.3' 'C.5.4'\n",
      " 'C.5.5' 'C.5.m' 'C.m' 'D.0' 'D.1' 'D.1.0' 'D.1.1' 'D.1.2' 'D.1.3' 'D.1.4'\n",
      " 'D.1.5' 'D.1.6' 'D.1.7' 'D.1.m' 'D.2' 'D.2.0' 'D.2.1' 'D.2.10' 'D.2.11'\n",
      " 'D.2.12' 'D.2.13' 'D.2.2' 'D.2.3' 'D.2.4' 'D.2.5' 'D.2.6' 'D.2.7' 'D.2.8'\n",
      " 'D.2.9' 'D.2.m' 'D.3' 'D.3.0' 'D.3.1' 'D.3.2' 'D.3.3' 'D.3.4' 'D.3.m'\n",
      " 'D.4' 'D.4.0' 'D.4.1' 'D.4.2' 'D.4.3' 'D.4.4' 'D.4.5' 'D.4.6' 'D.4.7'\n",
      " 'D.4.8' 'D.4.9' 'D.4.m' 'D.m' 'E.0' 'E.1' 'E.2' 'E.3' 'E.4' 'E.5' 'E.m'\n",
      " 'F.0' 'F.1' 'F.1.0' 'F.1.1' 'F.1.2' 'F.1.3' 'F.1.m' 'F.2' 'F.2.0' 'F.2.1'\n",
      " 'F.2.2' 'F.2.3' 'F.2.m' 'F.3' 'F.3.0' 'F.3.1' 'F.3.2' 'F.3.3' 'F.3.m'\n",
      " 'F.4' 'F.4.0' 'F.4.1' 'F.4.2' 'F.4.3' 'F.4.m' 'F.m' 'G.0' 'G.1' 'G.1.0'\n",
      " 'G.1.1' 'G.1.10' 'G.1.2' 'G.1.3' 'G.1.4' 'G.1.5' 'G.1.6' 'G.1.7' 'G.1.8'\n",
      " 'G.1.9' 'G.1.m' 'G.2' 'G.2.0' 'G.2.1' 'G.2.2' 'G.2.3' 'G.2.m' 'G.3' 'G.4'\n",
      " 'G.m' 'H.0' 'H.1' 'H.1.0' 'H.1.1' 'H.1.2' 'H.1.m' 'H.2' 'H.2.0' 'H.2.1'\n",
      " 'H.2.2' 'H.2.3' 'H.2.4' 'H.2.5' 'H.2.6' 'H.2.7' 'H.2.8' 'H.2.m' 'H.3'\n",
      " 'H.3.0' 'H.3.1' 'H.3.2' 'H.3.3' 'H.3.4' 'H.3.5' 'H.3.6' 'H.3.7' 'H.3.m'\n",
      " 'H.4' 'H.4.0' 'H.4.1' 'H.4.2' 'H.4.3' 'H.4.m' 'H.5' 'H.5.0' 'H.5.1'\n",
      " 'H.5.2' 'H.5.3' 'H.5.4' 'H.5.5' 'H.5.m' 'H.m' 'I.0' 'I.1' 'I.1.0' 'I.1.1'\n",
      " 'I.1.2' 'I.1.3' 'I.1.4' 'I.1.m' 'I.2' 'I.2.0' 'I.2.1' 'I.2.10' 'I.2.11'\n",
      " 'I.2.2' 'I.2.3' 'I.2.4' 'I.2.5' 'I.2.6' 'I.2.7' 'I.2.8' 'I.2.9' 'I.2.m'\n",
      " 'I.3' 'I.3.0' 'I.3.1' 'I.3.2' 'I.3.3' 'I.3.4' 'I.3.5' 'I.3.6' 'I.3.7'\n",
      " 'I.3.8' 'I.3.m' 'I.4' 'I.4.0' 'I.4.1' 'I.4.10' 'I.4.2' 'I.4.3' 'I.4.4'\n",
      " 'I.4.5' 'I.4.6' 'I.4.7' 'I.4.8' 'I.4.9' 'I.4.m' 'I.5' 'I.5.0' 'I.5.1'\n",
      " 'I.5.2' 'I.5.3' 'I.5.4' 'I.5.5' 'I.5.m' 'I.6' 'I.6.0' 'I.6.1' 'I.6.2'\n",
      " 'I.6.3' 'I.6.4' 'I.6.5' 'I.6.6' 'I.6.7' 'I.6.8' 'I.6.m' 'I.7' 'I.7.0'\n",
      " 'I.7.1' 'I.7.2' 'I.7.3' 'I.7.4' 'I.7.5' 'I.7.m' 'I.m' 'J' 'J.0' 'J.1'\n",
      " 'J.2' 'J.3' 'J.4' 'J.5' 'J.6' 'J.7' 'J.m' 'K.0' 'K.1' 'K.2' 'K.3' 'K.3.0'\n",
      " 'K.3.1' 'K.3.2' 'K.3.m' 'K.4' 'K.4.0' 'K.4.1' 'K.4.2' 'K.4.3' 'K.4.4'\n",
      " 'K.4.m' 'K.5' 'K.5.0' 'K.5.1' 'K.5.2' 'K.5.m' 'K.6' 'K.6.0' 'K.6.1'\n",
      " 'K.6.2' 'K.6.3' 'K.6.4' 'K.6.5' 'K.6.m' 'K.7' 'K.7.0' 'K.7.1' 'K.7.2'\n",
      " 'K.7.3' 'K.7.4' 'K.7.m' 'K.8' 'K.8.0' 'K.8.1' 'K.8.2' 'K.8.3' 'K.8.m'\n",
      " 'K.m']\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, joblib\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text    import CountVectorizer\n",
    "from sklearn.decomposition               import LatentDirichletAllocation\n",
    "from sklearn.preprocessing               import MultiLabelBinarizer\n",
    "\n",
    "MODELDIR = Path(\"models\")\n",
    "MODELDIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def load_or_train_vectorizer(train_text, *, path=MODELDIR/\"vectorizer.pkl\",\n",
    "                             **kwargs):\n",
    "    if path.exists():\n",
    "        print(\"Vectorizer loaded\")\n",
    "        return joblib.load(path)\n",
    "    print(\"training Vectorizer...\")\n",
    "    vec = CountVectorizer(stop_words=stop_words, max_df=0.95, min_df=2, **kwargs)\n",
    "    vec.fit(train_text)\n",
    "    joblib.dump(vec, path)\n",
    "    return vec\n",
    "\n",
    "def load_or_train_lda(train_matrix, *, path=MODELDIR/\"lda_model.pkl\",\n",
    "                      n_components=200, random_state=42, **kwargs):\n",
    "    if path.exists():\n",
    "        print(\"LDA loaded\")\n",
    "        return joblib.load(path)\n",
    "    print(\"training LDA...\")\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=random_state, **kwargs)\n",
    "    lda.fit(train_matrix)\n",
    "    joblib.dump(lda, path)\n",
    "    return lda\n",
    "\n",
    "def load_or_train_mlb(train_labels, *, path=MODELDIR/\"mlb.pkl\",\n",
    "                      all_labels=None):\n",
    "    if path.exists():\n",
    "        print(\"✓ MLB loaded\")\n",
    "        return joblib.load(path)\n",
    "    print(\"… training MultiLabelBinarizer\")\n",
    "    if all_labels is None:\n",
    "        all_labels = sorted({lbl for sub in train_labels for lbl in sub})\n",
    "    mlb = MultiLabelBinarizer(classes=all_labels)\n",
    "    mlb.fit(train_labels)\n",
    "    joblib.dump(mlb, path)\n",
    "    return mlb\n",
    "\n",
    "val   = train_full.iloc[80_000:].reset_index(drop=True)\n",
    "train = train_full.iloc[:80_000].reset_index(drop=True)\n",
    "print(\"Train:\", train.shape, \" Val:\", val.shape)\n",
    "\n",
    "\n",
    "vectorizer = load_or_train_vectorizer(train[\"Text\"])\n",
    "X_train_bow = vectorizer.transform(train[\"Text\"])\n",
    "X_val_bow   = vectorizer.transform(val[\"Text\"])\n",
    "X_test_bow  = vectorizer.transform(test[\"Text\"])\n",
    "\n",
    "\n",
    "lda = load_or_train_lda(X_train_bow)\n",
    "X_train_topics = lda.transform(X_train_bow)   # fit już zrobiony\n",
    "X_val_topics   = lda.transform(X_val_bow)\n",
    "X_test_topics  = lda.transform(X_test_bow)\n",
    "\n",
    "\n",
    "all_topics = sorted({lbl for sub in train_full[\"Topics\"] for lbl in sub})\n",
    "mlb = load_or_train_mlb(train[\"Topics\"], all_labels=all_topics)\n",
    "\n",
    "y_train = mlb.transform(train[\"Topics\"])\n",
    "y_val   = mlb.transform(val[\"Topics\"])\n",
    "\n",
    "print(\"Vectorizer vocab size:\", len(vectorizer.get_feature_names_out()))\n",
    "print(\"LDA topics:\", lda.n_components)\n",
    "print(\"y_train shape:\", y_train.shape)        # (80000, 358)\n",
    "print(\"y_val   shape:\", y_val.shape)          # (20000, 358)\n",
    "print(mlb.classes_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4ba14",
   "metadata": {},
   "source": [
    "# Check 5 topics distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49072bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "580106.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1755942.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1416298.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1516665.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1259693.txt",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8ad4be8f-9c6d-41c3-af0f-6c5a34dd4714",
       "rows": [
        [
         "Topic 0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 3",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 4",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 5",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 6",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 7",
         "0.128",
         "0.0",
         "0.043",
         "0.0",
         "0.0"
        ],
        [
         "Topic 8",
         "0.0",
         "0.0",
         "0.0",
         "0.048",
         "0.0"
        ],
        [
         "Topic 9",
         "0.013",
         "0.0",
         "0.0",
         "0.0",
         "0.07"
        ],
        [
         "Topic 10",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 11",
         "0.027",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 12",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.015"
        ],
        [
         "Topic 13",
         "0.037",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 14",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 15",
         "0.018",
         "0.0",
         "0.0",
         "0.056",
         "0.015"
        ],
        [
         "Topic 16",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.027"
        ],
        [
         "Topic 17",
         "0.0",
         "0.0",
         "0.053",
         "0.0",
         "0.0"
        ],
        [
         "Topic 18",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 19",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 20",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.068"
        ],
        [
         "Topic 21",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 22",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 23",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 24",
         "0.0",
         "0.0",
         "0.308",
         "0.109",
         "0.0"
        ],
        [
         "Topic 25",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 26",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 27",
         "0.0",
         "0.014",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 28",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 29",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 30",
         "0.0",
         "0.0",
         "0.0",
         "0.093",
         "0.0"
        ],
        [
         "Topic 31",
         "0.033",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 32",
         "0.0",
         "0.0",
         "0.04",
         "0.0",
         "0.0"
        ],
        [
         "Topic 33",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 34",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 35",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 36",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 37",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.024"
        ],
        [
         "Topic 38",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 39",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 40",
         "0.0",
         "0.026",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 41",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 42",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 43",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 44",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 45",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 46",
         "0.34",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 47",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 48",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 49",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 200
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>580106.txt</th>\n",
       "      <th>1755942.txt</th>\n",
       "      <th>1416298.txt</th>\n",
       "      <th>1516665.txt</th>\n",
       "      <th>1259693.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 195</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 196</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 198</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 199</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           580106.txt  1755942.txt  1416298.txt  1516665.txt  1259693.txt\n",
       "Topic 0           0.0          0.0          0.0          0.0          0.0\n",
       "Topic 1           0.0          0.0          0.0          0.0          0.0\n",
       "Topic 2           0.0          0.0          0.0          0.0          0.0\n",
       "Topic 3           0.0          0.0          0.0          0.0          0.0\n",
       "Topic 4           0.0          0.0          0.0          0.0          0.0\n",
       "...               ...          ...          ...          ...          ...\n",
       "Topic 195         0.0          0.0          0.0          0.0          0.0\n",
       "Topic 196         0.0          0.0          0.0          0.0          0.0\n",
       "Topic 197         0.0          0.0          0.0          0.0          0.0\n",
       "Topic 198         0.0          0.0          0.0          0.0          0.0\n",
       "Topic 199         0.0          0.0          0.0          0.0          0.0\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ids = train[\"Textfile\"].iloc[:5].values\n",
    "topic_distributions = X_train_topics[:5]\n",
    "\n",
    "topics_df = pd.DataFrame(np.round(topic_distributions, 3),\n",
    "                                   columns=[f\"Topic {i}\" for i in range(lda.n_components)],\n",
    "                                   index=file_ids)\n",
    "topics_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992e7a8",
   "metadata": {},
   "source": [
    "# Let's see words assigned to different topics with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea0754a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: sorting ensemble port hypercube interconnection parameterization boosting suffix ensembles transparency\n",
      "Topic 1: distribution estimation statistical estimates estimate statistics sample distributions likelihood variance\n",
      "Topic 2: mapping environment reality augmented environments urban real ar vr presence\n",
      "Topic 3: mechanism workflow mechanisms auction workflows agreement auctions party incentive elicitation\n",
      "Topic 4: air fabric injection pd pollution patch dilemma complexes simplicial catalog\n",
      "Topic 5: series hybrid term model long short forecasting forecast time weather\n",
      "Topic 6: management information identification tags rfid tag ir legal law compliance\n",
      "Topic 7: project risk projects software practices development agile management developers best\n",
      "Topic 8: process decision processes making model criteria hierarchical decisions approach selection\n",
      "Topic 9: grid multimedia media mobile applications devices paper based system computing\n",
      "Topic 10: codes code cyclic copy walking correcting gait dr error locomotion\n",
      "Topic 11: tool tools ada use debugging using benchmarking evaluation help used\n",
      "Topic 12: transaction concurrent transactions concurrency conflict atomic conflicts lock transactional synchronization\n",
      "Topic 13: wireless networks access layer network 802 mobile radio mac ieee\n",
      "Topic 14: cell production cells selective activity model neurons response effects cortex\n",
      "Topic 15: knowledge domain base attributes based conceptual domains attribute set representation\n",
      "Topic 16: graphics animation interactive 3d computer manipulation rendering animations animated editing\n",
      "Topic 17: availability available de http motivation www data genome edu supplementary\n",
      "Topic 18: free regular string grammar universal intersection grammars strings parsing normal\n",
      "Topic 19: ai narrative driver drivers ph shell persistence script crossing mit\n",
      "Topic 20: consistency consistent unification arc elementary notion operational interpretations properties equational\n",
      "Topic 21: table tables summarization summary lookup anonymity summaries closure transitive crawling\n",
      "Topic 22: formation co diffusion reaction barrier concentration growth deposition spectra ni\n",
      "Topic 23: agent agents multi based system intelligent preferences paper approach user\n",
      "Topic 24: architecture applications integration management system systems application support paper architectural\n",
      "Topic 25: key action encryption protocols secure actions cryptographic public one security\n",
      "Topic 26: virtual world environments environment real worlds physical virtualization machine indoor\n",
      "Topic 27: verification specification formal model state checking properties specifications abstract correctness\n",
      "Topic 28: organization organizational organizations structured taxonomy research systems unstructured technology change\n",
      "Topic 29: patterns pattern dependencies events bugs system bug violations race dynamic\n",
      "Topic 30: system storage file disk data performance files operating systems trace\n",
      "Topic 31: code java source compiler static dynamic runtime execution program compilation\n",
      "Topic 32: simulation system model controller control vehicle paper simulator using discrete\n",
      "Topic 33: voting flight nc system dp cp election unmanned using vote\n",
      "Topic 34: passive reactions inductive multilevel two actuators scm ce non based\n",
      "Topic 35: semi topic data supervised labels labeled label topics labeling active\n",
      "Topic 36: routing networks network ad hoc link topology performance nodes mobile\n",
      "Topic 37: know linux windows get os like would people one want\n",
      "Topic 38: ip protection property intellectual reference mpls layered resilience error fingerprinting\n",
      "Topic 39: protocol protocols edge mutual scheme authentication one new star consensus\n",
      "Topic 40: number set problem polynomial given show np prove every vertices\n",
      "Topic 41: length stack repair macro prefix copies spreading longest stacks virus\n",
      "Topic 42: image images resolution based method using fusion features results imaging\n",
      "Topic 43: order fractional delta skeleton higher 221e rm jump 03b3 conjecture\n",
      "Topic 44: temperature thermal micro layer high thin surface using materials silicon\n",
      "Topic 45: rules rule association missing incomplete data values based set sets\n",
      "Topic 46: service services web server based client paper applications servers application\n",
      "Topic 47: research social community work communities people researchers new future issues\n",
      "Topic 48: random variables variable sequences values number formula sampling correlation correlated\n",
      "Topic 49: simulated em annealing axis hyper ball sa algorithm medial landscape\n",
      "Topic 50: self map maps organizing reporting som relaxation spreadsheet nuclear cas\n",
      "Topic 51: migration viewpoint fidelity workspace vliw viewpoints asset steering modis peripheral\n",
      "Topic 52: compression algorithm template head vertical method using size division super\n",
      "Topic 53: visual visualization data graphical interactive visualizations visualizing techniques exploration visualize\n",
      "Topic 54: algorithm algorithms log time approximation distance space case two problem\n",
      "Topic 55: technology education computing computer university computers school teachers national educational\n",
      "Topic 56: complexity finite automata convex cellular timed class deterministic set time\n",
      "Topic 57: signal filter signals frequency phase filtering filters wavelet noise using\n",
      "Topic 58: segmentation region regions based image background method algorithm segment segments\n",
      "Topic 59: classification classifier classifiers svm support accuracy classify machine classifying based\n",
      "Topic 60: components system component framework application based software systems embedded applications\n",
      "Topic 61: point arithmetic transformations floating alpha simplification carry beta adder cam\n",
      "Topic 62: communication technology spectrum communications technologies use people computer study mediated\n",
      "Topic 63: path matrix paths matrices rank apl algorithm shortest sparse permutation\n",
      "Topic 64: session scratch age trigger matter mediation pad white rpc icons\n",
      "Topic 65: information web documents document users system user metadata browsing based\n",
      "Topic 66: face expressions facial recognition faces expression biometric opinion skin system\n",
      "Topic 67: digital public government expression gene biological information regulation evolution regulatory\n",
      "Topic 68: context location aware library information mobile based users libraries data\n",
      "Topic 69: planning expert navigation system plan intelligence mail spam phone obstacles\n",
      "Topic 70: voice sound strength fingerprint voip terrain arabic container pointing cr\n",
      "Topic 71: offline medium mis ads backup online identities behaviours identity sized\n",
      "Topic 72: language languages type types programming semantics natural definition system functional\n",
      "Topic 73: video videos surveillance encoding frames frame pruning shot mpeg shaping\n",
      "Topic 74: eye categories cancer genes binding gene biomedical score categorization using\n",
      "Topic 75: data spatial satellite water environmental study ground area using land\n",
      "Topic 76: function learning class algorithm dual classes gradient examples entropy new\n",
      "Topic 77: health care patients patient false monitoring copyright john medical wiley\n",
      "Topic 78: asynchronous clock branch synchronous line ring lines branches skew mixed\n",
      "Topic 79: protein sequence structure structural sequences contact dna proteins interactions molecular\n",
      "Topic 80: query queries xml data relational schema database processing views databases\n",
      "Topic 81: search query engine results web ranking searching engines queries retrieval\n",
      "Topic 82: market price equilibrium economic financial markets demand pricing investment competition\n",
      "Topic 83: numerical method equations linear order equation solution methods differential nonlinear\n",
      "Topic 84: hot mappings lists indirect spot vibration sea rs spots fatigue\n",
      "Topic 85: clustering data cluster method based clusters algorithm methods proposed approach\n",
      "Topic 86: software development engineering requirements product process case paper study research\n",
      "Topic 87: memory cache performance load shared caching access latency processor show\n",
      "Topic 88: sub music diagram musical slicing slice voronoi dependability slices latin\n",
      "Topic 89: feature features based word extraction using words selection training translation\n",
      "Topic 90: systems modelling system computer theory complex paper dynamical behaviour model\n",
      "Topic 91: power time supply synchronization chain consumption voltage savings saving dynamic\n",
      "Topic 92: fuzzy uncertainty proposed system based paper uncertain method approach robust\n",
      "Topic 93: policy policies access authorization round enforcement based si rbac role\n",
      "Topic 94: display light rendering depth view displays 3d ray using illumination\n",
      "Topic 95: mining data discovery patterns frequent algorithm algorithms approach paper discover\n",
      "Topic 96: method algorithm speech noise proposed based methods adaptive iterative new\n",
      "Topic 97: splitting step split membrane chart rc propagation stepping boltzmann cash\n",
      "Topic 98: stochastic generator generators method tensor fractal pseudo interior stationary exact\n",
      "Topic 99: network networks neural model based artificial results using paper qos\n",
      "Topic 100: motion tracking 3d camera using method based human body position\n",
      "Topic 101: problem problems solution solve solving solutions constraints optimization approach solved\n",
      "Topic 102: scheme channel rate coding transmission channels proposed performance interference error\n",
      "Topic 103: recognition temporal spatio model handwritten classes class accuracy performance two\n",
      "Topic 104: traffic packet network bandwidth end qos tcp ip congestion performance\n",
      "Topic 105: aspect concerns aspects separation refactoring automotive concern oriented modularity aop\n",
      "Topic 106: bridge ipv6 differentiation signaling ap handover tail intra bridges agricultural\n",
      "Topic 107: model probabilistic markov bayesian inference hidden models dependency probability based\n",
      "Topic 108: learning system based paper model learners learner educational environment development\n",
      "Topic 109: students computer course science student teaching learning courses study university\n",
      "Topic 110: matching shape similarity based shapes geometric geometry texture representation structure\n",
      "Topic 111: tv meeting shadow password television wikipedia drug passwords qualities wiki\n",
      "Topic 112: question module questions answer modules answers answering distortion 264 insurance\n",
      "Topic 113: logic reasoning theory operators logical calculus order boolean first semantics\n",
      "Topic 114: capacity diversity antenna directional array mimo bi antennas multiple publish\n",
      "Topic 115: medical update assembly cloud updates records clinical healthcare updating medicine\n",
      "Topic 116: errors soft annotation error annotations ambient go annotated notes stem\n",
      "Topic 117: semantic ontology concepts ontologies semantics i3 based concept semantically descriptions\n",
      "Topic 118: models model modeling complex used different based techniques mathematical analysis\n",
      "Topic 119: graph graphs children directed connected edges nodes edge connectivity drawing\n",
      "Topic 120: trust customer customers value consumer marketing product products relationships relationship\n",
      "Topic 121: hardware architecture processor performance fpga instruction high implementation processors reconfigurable\n",
      "Topic 122: detection recovery detect signature intrusion detecting signatures based detected anomaly\n",
      "Topic 123: objects event events activity object change changes activities nearest based\n",
      "Topic 124: content text audio items news entry item merging based system\n",
      "Topic 125: art state email immune artificial category painting artists centre ci\n",
      "Topic 126: force pc haptic forces gui emulation finger pcs clauses sap\n",
      "Topic 127: prediction model regression kernel methods error method accuracy data predict\n",
      "Topic 128: model object oriented modeling transformation driven approach meta based paper\n",
      "Topic 129: smart scan flip recording volume speculation laboratories flops clones clone\n",
      "Topic 130: quality performance measurement measures evaluation measure assessment maintenance system measuring\n",
      "Topic 131: interval dynamics stock intervals chaotic fold folding chaos volatility returns\n",
      "Topic 132: algorithm optimization algorithms genetic evolutionary objective proposed based results population\n",
      "Topic 133: transform determination method colour heavy infinite size displacement winner orbit\n",
      "Topic 134: time scheduling times job queue jobs priority queueing response arrival\n",
      "Topic 135: parallel performance applications computing large high cluster processors scale application\n",
      "Topic 136: particle swarm ant pso optimization algorithm colony particles based es\n",
      "Topic 137: adaptive constraint fine grained active cooperative model satisfaction csp based\n",
      "Topic 138: delay placement assignment timing delays layout algorithm based wire routing\n",
      "Topic 139: instrument acoustic sound auto sounds pathways auditory car guides pathway\n",
      "Topic 140: design system computer designers paper designing use user process used\n",
      "Topic 141: loop redundancy loops dependence iterations sided fan elimination lda ps\n",
      "Topic 142: predicates predicate lisp pagerank facets set facet drm rankings suggestion\n",
      "Topic 143: collaborative collaboration team work support teams awareness office participants members\n",
      "Topic 144: electronic commerce transportation model facility payment economy dss indicators system\n",
      "Topic 145: decoding receiver symbols check decoder detector symbol plasma detectors low\n",
      "Topic 146: user interaction interface human users interfaces interactive usability based paper\n",
      "Topic 147: program programming programs programmers use programmer execution language paper using\n",
      "Topic 148: polynomial algebraic polynomials symbolic root rational field multivariate degree roots\n",
      "Topic 149: sensor sensors localization aggregation sensing networks proximity key keys deployment\n",
      "Topic 150: game games play players player playing role gaming online played\n",
      "Topic 151: optimal cost problem bound lower bounds algorithm upper time size\n",
      "Topic 152: al et sets generalized set algorithm valued quasi vector weight\n",
      "Topic 153: schemes scheme replacement hash positioning gps hashing prefetching dht anonymous\n",
      "Topic 154: analysis web pages page name based names approach results analyses\n",
      "Topic 155: energy scaling variations consumption variation leakage power reduction variability bus\n",
      "Topic 156: control uml diagrams access controlled controls controlling system inf diagram\n",
      "Topic 157: handling exception exceptions spatiotemporal specialization dynamic zones magic observability spacecraft\n",
      "Topic 158: scientific scientists goals explanation representations nasa military research mission science\n",
      "Topic 159: conference papers workshop international center held 2005 2008 2007 2006\n",
      "Topic 160: group groups spaces space bases invariants algebra canonical causal invariant\n",
      "Topic 161: processing message messages passing exchange communication deadlock send pass one\n",
      "Topic 162: character safety style characters styles writer stroke system critical based\n",
      "Topic 163: strategies strategy trading emotion emotional affective emotions late plagiarism ips\n",
      "Topic 164: book web learn microsoft guide using use applications step new\n",
      "Topic 165: distributed peer p2p network system systems sharing based scalable peers\n",
      "Topic 166: surface mesh surfaces curve curves points method reconstruction meshes geometric\n",
      "Topic 167: study users user factors online perceived results survey site web\n",
      "Topic 168: 00bf right left sign cycle 2n omega hole signs le\n",
      "Topic 169: business enterprise management industry chapter book technology technical technologies new\n",
      "Topic 170: internet coordination overlay bgp probing topology route measurements overlays end\n",
      "Topic 171: time real analysis temporal model timing based paper fire run\n",
      "Topic 172: yield manufacturing defects defect semiconductor process response ic wafer compaction\n",
      "Topic 173: test testing tests generation coverage based cases diagnosis results method\n",
      "Topic 174: flow flows fluid model boundary method velocity pressure numerical simulations\n",
      "Topic 175: quantum convergence classical projection points correction theory point perturbation spherical\n",
      "Topic 176: level design synthesis chip designs high methodology soc interconnect performance\n",
      "Topic 177: block threshold color proposed embedding based blocks scheme watermarking method\n",
      "Topic 178: data spatial index stream large processing streams efficient structure structures\n",
      "Topic 179: security mobile secure devices computing authentication ubiquitous pervasive paper applications\n",
      "Topic 180: functions function probability boolean sp balanced repetitive neighbourhood lr construction\n",
      "Topic 181: fault reliability metrics failure failures tolerance tolerant faults system faulty\n",
      "Topic 182: retrieval feedback based sat relevance system tier performance paging sports\n",
      "Topic 183: stage switch input output sort stages switches two first system\n",
      "Topic 184: road blind box stories visually story impaired disaster boxes multilingual\n",
      "Topic 185: collection incremental garbage live pointer heap malware collector pointers bad\n",
      "Topic 186: software attacks attack malicious reverse security vulnerabilities system techniques analysis\n",
      "Topic 187: tree trees decomposition minimum spanning algorithm steiner node structure join\n",
      "Topic 188: optical multicast wavelength conversion switching burst wdm drop based probe\n",
      "Topic 189: recursive carlo monte symmetry circular recursion plate method damping expansion\n",
      "Topic 190: nodes sensor node network networks wireless energy broadcast data lifetime\n",
      "Topic 191: scheduling resource task allocation tasks resources time problem algorithm partitioning\n",
      "Topic 192: circuit circuits current voltage cmos low mode analog frequency gate\n",
      "Topic 193: database privacy databases data sql integrity preserving oracle information access\n",
      "Topic 194: robot robots robotic double robotics dns perception mismatch autonomous animal\n",
      "Topic 195: computation operations parallel algorithms implementation computations parallelism machine algorithm efficient\n",
      "Topic 196: cognitive tissue duration surgery surgical cisco bone assisted typing muscle\n",
      "Topic 197: element finite elements method analysis model inc wiley periodicals using\n",
      "Topic 198: brain 2009 stimuli 0394 resonance ct stimulus potentials mri fmri\n",
      "Topic 199: net nets petri chinese triangulation seed mathcal delaunay bank gamma\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def show_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx}: {' '.join(top_features)}\")\n",
    "\n",
    "show_top_words(lda, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0578de",
   "metadata": {},
   "source": [
    "# Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83e708be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to train classifier first...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "if os.path.exists(\"models/classifier.pkl\"):\n",
    "    print(\"Found classifier model!\")\n",
    "\n",
    "    with open(\"models/classifier.pkl\", \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print(\"We need to train classifier first...\")\n",
    "    lr= LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        solver=\"saga\",\n",
    "        n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # 3. Predykcja\n",
    "    clf = OneVsRestClassifier(lr, n_jobs=-1)\n",
    "    clf.fit(X_train_topics, y_train)\n",
    "\n",
    "    with open(\"models/classifier.pkl\", \"wb\") as f:\n",
    "        pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a1a5b",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dd58f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation...\n",
      "[['H.2.8'], ['C.2.1', 'C.2.2', 'C.4'], ['I.2.8'], ['G.1.8', 'J.2'], [], ['D.2.4', 'D.2.5'], [], [], [], [], ['I.4.6'], ['C.2.0', 'K.6.5'], [], [], [], ['K.3.2'], [], [], ['D.2.5'], ['I.2.11'], [], ['G.1.6'], [], [], [], [], [], [], [], [], [], [], [], ['K.3.1'], [], [], [], [], [], ['J.2'], [], ['G.2.2'], [], [], ['F.4.2'], [], [], [], [], ['K.3.2'], [], [], [], ['I.2.3'], [], [], ['H.3.3'], [], [], [], [], [], [], [], ['I.3.5'], [], [], [], ['C.2.1'], [], ['C.2.1', 'C.2.2'], ['I.4.8'], [], [], [], [], [], ['I.5.3'], [], ['I.3.5'], [], [], [], ['I.5.2'], [], [], ['F.2.2', 'G.2.1', 'G.2.2'], ['K.3.2'], [], ['H.5.2'], ['G.3'], ['F.2.2'], [], [], [], ['D.2.4'], [], ['H.2.4'], [], []]\n",
      "micro-F1 : 0.16725019364833463\n",
      "macro-F1 : 0.04226630834676749\n",
      "Hamming  : 0.00750754189944134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstanty/Projects/UW/UWvenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, hamming_loss\n",
    "\n",
    "print(\"Validation...\")\n",
    "y_pred_bin = clf.predict(X_val_topics)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "predicted_topics_list = [list(labels) for labels in y_pred_labels]\n",
    "print(predicted_topics_list[:100])\n",
    "\n",
    "val[\"PredictedTopics\"] = predicted_topics_list\n",
    "\n",
    "y_val_true_bin = mlb.transform(val[\"Topics\"])\n",
    "y_val_pred_bin = mlb.transform(val[\"PredictedTopics\"])\n",
    "\n",
    "\n",
    "print(\"micro-F1 :\", f1_score(y_val_true_bin, y_val_pred_bin, average=\"micro\"))\n",
    "print(\"macro-F1 :\", f1_score(y_val_true_bin, y_val_pred_bin, average=\"macro\"))\n",
    "print(\"Hamming  :\", hamming_loss(y_val_true_bin, y_val_pred_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a235947",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b07d3c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction...\n",
      "['H.5.2']\n",
      "['I.3.5']\n",
      "['I.2.11']\n",
      "['K.6.1']\n",
      "['I.2.8']\n",
      "['B.7.1', 'B.8.2']\n",
      "['K.3.2']\n",
      "['K.3.2']\n",
      "       Textfile Predicted Topics\n",
      "0    963168.txt               []\n",
      "1   1811004.txt               []\n",
      "2    192631.txt          [H.5.2]\n",
      "3   1183872.txt               []\n",
      "4   1280491.txt               []\n",
      "5   1059284.txt               []\n",
      "6   1133457.txt               []\n",
      "7   1140350.txt               []\n",
      "8    100973.txt               []\n",
      "9   1147150.txt               []\n",
      "10   598535.txt               []\n",
      "11  1318072.txt               []\n",
      "12  1222053.txt               []\n",
      "13   110442.txt               []\n",
      "14  1044226.txt          [I.3.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prediction...\")\n",
    "y_pred_bin = clf.predict(X_test_topics)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "# predicted_topics_str = [\",\".join(labels) if labels else \"-\" for labels in y_pred_labels]\n",
    "predicted_topics_list = [list(labels) for labels in y_pred_labels]\n",
    "for topics in predicted_topics_list[:50]:\n",
    "    if topics != []:\n",
    "        print(topics)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Textfile\": test[\"Textfile\"].values,\n",
    "    \"Predicted Topics\": predicted_topics_list\n",
    "})\n",
    "\n",
    "\n",
    "# Making sure this has the same order\n",
    "order = test[\"Textfile\"]\n",
    "\n",
    "\n",
    "results_sorted = (\n",
    "    results.set_index(\"Textfile\")   # <- klucz do dopasowania\n",
    "           .loc[order]              # <- reindex wg referencyjnej kolejności\n",
    "           .reset_index()           # <- wróć do zwykłej kolumny\n",
    ")\n",
    "\n",
    "# 3. (opcjonalnie) nadpisz `results`\n",
    "results = results_sorted\n",
    "\n",
    "print(results.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
