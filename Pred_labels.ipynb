{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a802eae",
   "metadata": {},
   "source": [
    "Data for this project consists of two tables in a tab-separated columns format. Each row in those files corresponds to an abstract of a scientific article from ACM Digital Library, which was assigned to one or more topics from the  ACM Computing Classification System (the old one from 1998).\n",
    "\n",
    "There are two data sets for this task, the training and the testing sample, respectively. They are text (TSV - tab separated values) files compressed using 7zip.\n",
    "\n",
    "The training data (DM2023_training_docs_and_labels.tsv) has three columns: the first one is an identifier of a document, the second one stores the text of the abstract, and the third one contains a list of comma-separated topic labels.\n",
    "\n",
    "The test data (DM2023_test_docs.tsv) has a similar format, but the labels in the third column are missing.\n",
    "\n",
    "**The task and the format of submissions:** the task for you is to predict the labels of documents from the test data and submit them to the moodle using the link below. A correctly formatted submission should be a text file with exactly 100000 lines plus the report. Each line should correspond to a document from the test data set (the order matters!) and contain a list of one or more predicted labels, separated by commas. The report can be in the form of R/Python notebook (with code and computation outcomes). Please remember about explanations and visualizations – make this report as interesting for a reader as you can.\n",
    "\n",
    "You may make several submissions (up to 20), so please remember to clearly mark the final version of your answer in case there is more than one.\n",
    "\n",
    "Practical note: The submission size in moodle is limited to 512MB. In case your files are larger please use compression (7z,gz, ...) other than Zip. Moodle does not like .zip files. \n",
    "\n",
    "Evaluation: the quality of submissions will be evaluated using a script to compute the average F1-score measure, i.e., for each test document, the F1-score between the predicted and true labels will be computed, and the values obtained for all test cases will be averaged.\n",
    "\n",
    "The deadline for sending the reports is Sunday, June 15.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf63f5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Categorizing documents in **ACM Digital Library** example: \n",
    "\n",
    "(Main Class).(Subclass).(Subsubcategory)\n",
    "\n",
    "**H.3.5**\n",
    "\n",
    "* H. Information Systems\n",
    "    *  H.3 Information Storage and Retrieval\n",
    "        * H.3.5 Online Information Services\n",
    "\n",
    "**D.3.2**\n",
    "\n",
    "* D. Software\n",
    "    * D.3 Programming Languages\n",
    "        * D.3.2 Language Classifications\n",
    "  \n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd6ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:16:59.198249Z",
     "start_time": "2025-06-01T14:16:56.074333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/konstanty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba unikalnych tematów: 358\n",
      "First 10 example topics:  ['A.0', 'A.1', 'A.2', 'A.m', 'B.0', 'B.1', 'B.1.0', 'B.1.1', 'B.1.2', 'B.1.3']\n"
     ]
    }
   ],
   "source": [
    "# import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "test = pd.read_csv(\"data/DM2023_test_docs.tsv\", \n",
    "                    sep=\"\\t\", \n",
    "                    encoding=\"latin1\", \n",
    "                    header=None,\n",
    "                    names=[\"Textfile\", \"Text\", \"Topics\"])\n",
    "test = test.drop_duplicates()\n",
    "                    \n",
    "                    \n",
    "train = pd.read_csv(\"data/DM2023_training_docs_and_labels.tsv\", \n",
    "                    sep=\"\\t\", \n",
    "                    encoding=\"latin1\", \n",
    "                    header=None,\n",
    "                    names=[\"Textfile\", \"Text\", \"Topics\"])\n",
    "\n",
    "\n",
    "def flatten_if_single(x):\n",
    "    \"\"\"Jeśli x jest listą długości 1 – zwróć jej pierwszy element.\"\"\"\n",
    "    if isinstance(x, list) and len(x) == 1:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "# Separating topics\n",
    "train[\"Topics\"] = (\n",
    "    train[\"Topics\"]\n",
    "    .apply(flatten_if_single)        \n",
    "    .str.split(r\"\\s*,\\s*\")         \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# train[\"Topics\"] = train[\"Topics\"].str.split(\",\")\n",
    "\n",
    "unique_labels = set(label for sublist in train[\"Topics\"] for label in sublist)\n",
    "\n",
    "print(f\"Number of unique topics: {len(unique_labels)}\")\n",
    "print(\"First 10 example topics: \",sorted(list(unique_labels))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7af26a",
   "metadata": {},
   "source": [
    "# Train LDA, MLB (Or load) and topic distribution \n",
    "(Shape of mlb binary matrix should match the number of unique topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3062a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to train LDA, MLB and Vectorizer...\n",
      "Number of unique topics (ex. 'A.5' or 'H.3.5'): 358\n",
      "Shape y_train: (100000, 358)\n",
      "['A.0' 'A.1' 'A.2' 'A.m' 'B.0' 'B.1' 'B.1.0' 'B.1.1' 'B.1.2' 'B.1.3'\n",
      " 'B.1.4' 'B.1.5' 'B.1.m' 'B.2' 'B.2.0' 'B.2.1' 'B.2.2' 'B.2.3' 'B.2.4'\n",
      " 'B.2.m' 'B.3' 'B.3.0' 'B.3.1' 'B.3.2' 'B.3.3' 'B.3.4' 'B.3.m' 'B.4'\n",
      " 'B.4.0' 'B.4.1' 'B.4.2' 'B.4.3' 'B.4.4' 'B.4.5' 'B.4.m' 'B.5' 'B.5.0'\n",
      " 'B.5.1' 'B.5.2' 'B.5.3' 'B.5.m' 'B.6' 'B.6.0' 'B.6.1' 'B.6.2' 'B.6.3'\n",
      " 'B.6.m' 'B.7' 'B.7.0' 'B.7.1' 'B.7.2' 'B.7.3' 'B.7.m' 'B.8' 'B.8.0'\n",
      " 'B.8.1' 'B.8.2' 'B.8.m' 'B.m' 'C.0' 'C.1' 'C.1.0' 'C.1.1' 'C.1.2' 'C.1.3'\n",
      " 'C.1.4' 'C.1.m' 'C.2' 'C.2.0' 'C.2.1' 'C.2.2' 'C.2.3' 'C.2.4' 'C.2.5'\n",
      " 'C.2.6' 'C.2.m' 'C.3' 'C.4' 'C.5' 'C.5.0' 'C.5.1' 'C.5.2' 'C.5.3' 'C.5.4'\n",
      " 'C.5.5' 'C.5.m' 'C.m' 'D.0' 'D.1' 'D.1.0' 'D.1.1' 'D.1.2' 'D.1.3' 'D.1.4'\n",
      " 'D.1.5' 'D.1.6' 'D.1.7' 'D.1.m' 'D.2' 'D.2.0' 'D.2.1' 'D.2.10' 'D.2.11'\n",
      " 'D.2.12' 'D.2.13' 'D.2.2' 'D.2.3' 'D.2.4' 'D.2.5' 'D.2.6' 'D.2.7' 'D.2.8'\n",
      " 'D.2.9' 'D.2.m' 'D.3' 'D.3.0' 'D.3.1' 'D.3.2' 'D.3.3' 'D.3.4' 'D.3.m'\n",
      " 'D.4' 'D.4.0' 'D.4.1' 'D.4.2' 'D.4.3' 'D.4.4' 'D.4.5' 'D.4.6' 'D.4.7'\n",
      " 'D.4.8' 'D.4.9' 'D.4.m' 'D.m' 'E.0' 'E.1' 'E.2' 'E.3' 'E.4' 'E.5' 'E.m'\n",
      " 'F.0' 'F.1' 'F.1.0' 'F.1.1' 'F.1.2' 'F.1.3' 'F.1.m' 'F.2' 'F.2.0' 'F.2.1'\n",
      " 'F.2.2' 'F.2.3' 'F.2.m' 'F.3' 'F.3.0' 'F.3.1' 'F.3.2' 'F.3.3' 'F.3.m'\n",
      " 'F.4' 'F.4.0' 'F.4.1' 'F.4.2' 'F.4.3' 'F.4.m' 'F.m' 'G.0' 'G.1' 'G.1.0'\n",
      " 'G.1.1' 'G.1.10' 'G.1.2' 'G.1.3' 'G.1.4' 'G.1.5' 'G.1.6' 'G.1.7' 'G.1.8'\n",
      " 'G.1.9' 'G.1.m' 'G.2' 'G.2.0' 'G.2.1' 'G.2.2' 'G.2.3' 'G.2.m' 'G.3' 'G.4'\n",
      " 'G.m' 'H.0' 'H.1' 'H.1.0' 'H.1.1' 'H.1.2' 'H.1.m' 'H.2' 'H.2.0' 'H.2.1'\n",
      " 'H.2.2' 'H.2.3' 'H.2.4' 'H.2.5' 'H.2.6' 'H.2.7' 'H.2.8' 'H.2.m' 'H.3'\n",
      " 'H.3.0' 'H.3.1' 'H.3.2' 'H.3.3' 'H.3.4' 'H.3.5' 'H.3.6' 'H.3.7' 'H.3.m'\n",
      " 'H.4' 'H.4.0' 'H.4.1' 'H.4.2' 'H.4.3' 'H.4.m' 'H.5' 'H.5.0' 'H.5.1'\n",
      " 'H.5.2' 'H.5.3' 'H.5.4' 'H.5.5' 'H.5.m' 'H.m' 'I.0' 'I.1' 'I.1.0' 'I.1.1'\n",
      " 'I.1.2' 'I.1.3' 'I.1.4' 'I.1.m' 'I.2' 'I.2.0' 'I.2.1' 'I.2.10' 'I.2.11'\n",
      " 'I.2.2' 'I.2.3' 'I.2.4' 'I.2.5' 'I.2.6' 'I.2.7' 'I.2.8' 'I.2.9' 'I.2.m'\n",
      " 'I.3' 'I.3.0' 'I.3.1' 'I.3.2' 'I.3.3' 'I.3.4' 'I.3.5' 'I.3.6' 'I.3.7'\n",
      " 'I.3.8' 'I.3.m' 'I.4' 'I.4.0' 'I.4.1' 'I.4.10' 'I.4.2' 'I.4.3' 'I.4.4'\n",
      " 'I.4.5' 'I.4.6' 'I.4.7' 'I.4.8' 'I.4.9' 'I.4.m' 'I.5' 'I.5.0' 'I.5.1'\n",
      " 'I.5.2' 'I.5.3' 'I.5.4' 'I.5.5' 'I.5.m' 'I.6' 'I.6.0' 'I.6.1' 'I.6.2'\n",
      " 'I.6.3' 'I.6.4' 'I.6.5' 'I.6.6' 'I.6.7' 'I.6.8' 'I.6.m' 'I.7' 'I.7.0'\n",
      " 'I.7.1' 'I.7.2' 'I.7.3' 'I.7.4' 'I.7.5' 'I.7.m' 'I.m' 'J' 'J.0' 'J.1'\n",
      " 'J.2' 'J.3' 'J.4' 'J.5' 'J.6' 'J.7' 'J.m' 'K.0' 'K.1' 'K.2' 'K.3' 'K.3.0'\n",
      " 'K.3.1' 'K.3.2' 'K.3.m' 'K.4' 'K.4.0' 'K.4.1' 'K.4.2' 'K.4.3' 'K.4.4'\n",
      " 'K.4.m' 'K.5' 'K.5.0' 'K.5.1' 'K.5.2' 'K.5.m' 'K.6' 'K.6.0' 'K.6.1'\n",
      " 'K.6.2' 'K.6.3' 'K.6.4' 'K.6.5' 'K.6.m' 'K.7' 'K.7.0' 'K.7.1' 'K.7.2'\n",
      " 'K.7.3' 'K.7.4' 'K.7.m' 'K.8' 'K.8.0' 'K.8.1' 'K.8.2' 'K.8.3' 'K.8.m'\n",
      " 'K.m']\n"
     ]
    }
   ],
   "source": [
    "# Sparse matrix with col=words, row=word count in all documents\n",
    "import pickle\n",
    "import os.path\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "\n",
    "if os.path.exists(\"models/lda_model.pkl\") and os.path.exists(\"models/vectorizer.pkl\") and os.path.exists(\"models/mlb_model.pkl\"):\n",
    "\n",
    "    print(\"Found LDA and vectorizer models!\")\n",
    "    \n",
    "    with open(\"models/lda_model.pkl\", \"rb\") as f:\n",
    "        lda = pickle.load(f)\n",
    "\n",
    "    with open(\"models/vectorizer.pkl\", \"rb\") as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "\n",
    "    with open(\"models/mlb_model.pkl\", \"rb\") as f:\n",
    "        mlb = pickle.load(f)\n",
    "\n",
    "    X_train = vectorizer.transform(train[\"Text\"])\n",
    "    X_train = lda.transform(X_train)\n",
    "    y_train = mlb.transform(train[\"Topics\"])\n",
    "    print(f\"Number of unique topics (ex. 'A.5' or 'H.3.5'): {len(mlb.classes_)}\")\n",
    "    print(f\"Shape y_train: {y_train.shape}\")\n",
    "\n",
    "    X_test = vectorizer.transform(train[\"Text\"])\n",
    "    X_test = lda.transform(X_test)\n",
    "\n",
    "else:\n",
    "    print(\"We need to train LDA, MLB and Vectorizer...\")\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words, max_df=0.95, min_df=2)\n",
    "    lda = LatentDirichletAllocation(n_components=50, random_state=42)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train[\"Text\"])\n",
    "    X_train = lda.fit_transform(X_train)\n",
    "    y_train = mlb.fit_transform(train[\"Topics\"])\n",
    "\n",
    "    print(f\"Number of unique topics (ex. 'A.5' or 'H.3.5'): {len(mlb.classes_)}\")\n",
    "    print(f\"Shape y_train: {y_train.shape}\")\n",
    "    \n",
    "    X_test = vectorizer.transform(train[\"Text\"])\n",
    "    X_test = lda.transform(X_test)\n",
    "\n",
    "    # Save models\n",
    "    with open(\"models/lda_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(lda, f)\n",
    "\n",
    "    with open(\"models/vectorizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "    with open(\"models/mlb_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(mlb, f)\n",
    "\n",
    "\n",
    "# print(mlb.classes_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4ba14",
   "metadata": {},
   "source": [
    "# Check 5 topics distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49072bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "580106.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1755942.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1416298.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1516665.txt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1259693.txt",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9c268846-2bc8-464d-8003-4451d69c146b",
       "rows": [
        [
         "Topic 0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 1",
         "0.0",
         "0.0",
         "0.019",
         "0.0",
         "0.467"
        ],
        [
         "Topic 2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 3",
         "0.018",
         "0.196",
         "0.0",
         "0.0",
         "0.171"
        ],
        [
         "Topic 4",
         "0.0",
         "0.0",
         "0.0",
         "0.149",
         "0.0"
        ],
        [
         "Topic 5",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 6",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 7",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.014"
        ],
        [
         "Topic 8",
         "0.0",
         "0.01",
         "0.3",
         "0.0",
         "0.0"
        ],
        [
         "Topic 9",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.014"
        ],
        [
         "Topic 10",
         "0.045",
         "0.0",
         "0.0",
         "0.151",
         "0.0"
        ],
        [
         "Topic 11",
         "0.0",
         "0.038",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 12",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 13",
         "0.0",
         "0.0",
         "0.331",
         "0.204",
         "0.0"
        ],
        [
         "Topic 14",
         "0.0",
         "0.071",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 15",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 16",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 17",
         "0.034",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 18",
         "0.05",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 19",
         "0.0",
         "0.0",
         "0.16",
         "0.0",
         "0.07"
        ],
        [
         "Topic 20",
         "0.518",
         "0.22",
         "0.092",
         "0.0",
         "0.0"
        ],
        [
         "Topic 21",
         "0.0",
         "0.078",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 22",
         "0.038",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 23",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 24",
         "0.0",
         "0.035",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 25",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 26",
         "0.073",
         "0.078",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 27",
         "0.0",
         "0.061",
         "0.0",
         "0.0",
         "0.144"
        ],
        [
         "Topic 28",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 29",
         "0.0",
         "0.013",
         "0.019",
         "0.0",
         "0.0"
        ],
        [
         "Topic 30",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 31",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 32",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.046"
        ],
        [
         "Topic 33",
         "0.0",
         "0.0",
         "0.0",
         "0.107",
         "0.0"
        ],
        [
         "Topic 34",
         "0.0",
         "0.072",
         "0.0",
         "0.099",
         "0.0"
        ],
        [
         "Topic 35",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 36",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 37",
         "0.217",
         "0.0",
         "0.0",
         "0.0",
         "0.052"
        ],
        [
         "Topic 38",
         "0.0",
         "0.03",
         "0.0",
         "0.278",
         "0.0"
        ],
        [
         "Topic 39",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 40",
         "0.0",
         "0.0",
         "0.069",
         "0.0",
         "0.0"
        ],
        [
         "Topic 41",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 42",
         "0.0",
         "0.069",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 43",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 44",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 45",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 46",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 47",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 48",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "Topic 49",
         "0.0",
         "0.024",
         "0.0",
         "0.0",
         "0.012"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 50
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>580106.txt</th>\n",
       "      <th>1755942.txt</th>\n",
       "      <th>1416298.txt</th>\n",
       "      <th>1516665.txt</th>\n",
       "      <th>1259693.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 14</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 15</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 16</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 17</th>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 18</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 19</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 20</th>\n",
       "      <td>0.518</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 21</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 22</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 23</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 24</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 25</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 26</th>\n",
       "      <td>0.073</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 27</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 28</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 29</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 30</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 31</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 32</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 33</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 34</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 35</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 36</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 37</th>\n",
       "      <td>0.217</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 38</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 39</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 40</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 41</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 42</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 43</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 44</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 45</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 46</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 47</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 48</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 49</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          580106.txt  1755942.txt  1416298.txt  1516665.txt  1259693.txt\n",
       "Topic 0        0.000        0.000        0.000        0.000        0.000\n",
       "Topic 1        0.000        0.000        0.019        0.000        0.467\n",
       "Topic 2        0.000        0.000        0.000        0.000        0.000\n",
       "Topic 3        0.018        0.196        0.000        0.000        0.171\n",
       "Topic 4        0.000        0.000        0.000        0.149        0.000\n",
       "Topic 5        0.000        0.000        0.000        0.000        0.000\n",
       "Topic 6        0.000        0.000        0.000        0.000        0.000\n",
       "Topic 7        0.000        0.000        0.000        0.000        0.014\n",
       "Topic 8        0.000        0.010        0.300        0.000        0.000\n",
       "Topic 9        0.000        0.000        0.000        0.000        0.014\n",
       "Topic 10       0.045        0.000        0.000        0.151        0.000\n",
       "Topic 11       0.000        0.038        0.000        0.000        0.000\n",
       "Topic 12       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 13       0.000        0.000        0.331        0.204        0.000\n",
       "Topic 14       0.000        0.071        0.000        0.000        0.000\n",
       "Topic 15       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 16       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 17       0.034        0.000        0.000        0.000        0.000\n",
       "Topic 18       0.050        0.000        0.000        0.000        0.000\n",
       "Topic 19       0.000        0.000        0.160        0.000        0.070\n",
       "Topic 20       0.518        0.220        0.092        0.000        0.000\n",
       "Topic 21       0.000        0.078        0.000        0.000        0.000\n",
       "Topic 22       0.038        0.000        0.000        0.000        0.000\n",
       "Topic 23       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 24       0.000        0.035        0.000        0.000        0.000\n",
       "Topic 25       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 26       0.073        0.078        0.000        0.000        0.000\n",
       "Topic 27       0.000        0.061        0.000        0.000        0.144\n",
       "Topic 28       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 29       0.000        0.013        0.019        0.000        0.000\n",
       "Topic 30       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 31       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 32       0.000        0.000        0.000        0.000        0.046\n",
       "Topic 33       0.000        0.000        0.000        0.107        0.000\n",
       "Topic 34       0.000        0.072        0.000        0.099        0.000\n",
       "Topic 35       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 36       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 37       0.217        0.000        0.000        0.000        0.052\n",
       "Topic 38       0.000        0.030        0.000        0.278        0.000\n",
       "Topic 39       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 40       0.000        0.000        0.069        0.000        0.000\n",
       "Topic 41       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 42       0.000        0.069        0.000        0.000        0.000\n",
       "Topic 43       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 44       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 45       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 46       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 47       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 48       0.000        0.000        0.000        0.000        0.000\n",
       "Topic 49       0.000        0.024        0.000        0.000        0.012"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ids = train[\"Textfile\"].iloc[:5].values\n",
    "topic_distributions = X_train[:5]\n",
    "\n",
    "topics_df = pd.DataFrame(np.round(topic_distributions, 3),\n",
    "                                   columns=[f\"Topic {i}\" for i in range(lda.n_components)],\n",
    "                                   index=file_ids)\n",
    "topics_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992e7a8",
   "metadata": {},
   "source": [
    "# Let's see words assigned to different topics with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0754a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: design hardware architecture processor performance embedded system level instruction designs\n",
      "Topic 1: user virtual system interface interaction interactive visual visualization display using\n",
      "Topic 2: translation optical language system english machine word wavelength reputation using\n",
      "Topic 3: security key protocol secure privacy access attacks based scheme protocols\n",
      "Topic 4: semantic information domain knowledge based model ontology representation concepts medical\n",
      "Topic 5: scheduling time cache performance parallel execution task tasks program analysis\n",
      "Topic 6: coding layer high layers compression si surface temperature thin silicon\n",
      "Topic 7: functions function linear set given space paper also polynomial one\n",
      "Topic 8: network networks traffic packet internet end ip bandwidth video qos\n",
      "Topic 9: algorithms methods matrix matrices algorithm sparse linear iterative problems decomposition\n",
      "Topic 10: database query queries xml data databases relational processing schema sql\n",
      "Topic 11: product market products customer value price cost customers financial costs\n",
      "Topic 12: detection fault faults failure error based errors time detect tolerance\n",
      "Topic 13: distributed system agent peer agents storage systems grid resource file\n",
      "Topic 14: study social information results users research user use factors group\n",
      "Topic 15: research development software information project technology systems engineering design issues\n",
      "Topic 16: codes vector code kernel placement selection problem linear support dual\n",
      "Topic 17: computer students science course university education teaching student computing courses\n",
      "Topic 18: mobile system devices services systems location computing service user paper\n",
      "Topic 19: software service management systems system business process based framework requirements\n",
      "Topic 20: web applications services application server platform java service client grid\n",
      "Topic 21: video 3d surface based shape objects object image algorithm motion\n",
      "Topic 22: learning knowledge based expert task system process paper learn approach\n",
      "Topic 23: neural network networks using water results used artificial based training\n",
      "Topic 24: memory performance time parallel applications system implementation high real systems\n",
      "Topic 25: signal noise proposed frequency adaptive filter based channel method performance\n",
      "Topic 26: language model programming system program object code programs oriented software\n",
      "Topic 27: model human models decision behavior analysis systems modeling robot making\n",
      "Topic 28: test testing patterns tests pattern software generation based cases coverage\n",
      "Topic 29: networks routing network nodes wireless sensor node protocol ad based\n",
      "Topic 30: image method images analysis based using component reconstruction imaging fusion\n",
      "Topic 31: method numerical equations order finite equation flow solution element boundary\n",
      "Topic 32: digital collaborative multimedia system collaboration users library media paper information\n",
      "Topic 33: data clustering mining large sets set based analysis algorithms approach\n",
      "Topic 34: gene sequence sequences protein expression biological genes contact available alignment\n",
      "Topic 35: image images color 00bf rfid tags tag visual based proposed\n",
      "Topic 36: graph algorithm graphs problem time tree number log algorithms two\n",
      "Topic 37: game games book people one computer children world new use\n",
      "Topic 38: web information search retrieval text document documents based user content\n",
      "Topic 39: control system tracking robot model motion vehicle position using controller\n",
      "Topic 40: power energy circuit design circuits low consumption voltage chip high\n",
      "Topic 41: algorithm arithmetic sequential computation calculus implementation parallel multiplication modular operations\n",
      "Topic 42: complexity constraints constraint show logic problem class quantum theory problems\n",
      "Topic 43: classification recognition based feature features method using proposed matching algorithm\n",
      "Topic 44: problem algorithm optimization problems search algorithms solution genetic optimal solutions\n",
      "Topic 45: spatial temporal data map analysis time maps study information series\n",
      "Topic 46: model models distribution estimation random probability statistical parameters probabilistic distributions\n",
      "Topic 47: simulation fuzzy system model process based modeling rules rule paper\n",
      "Topic 48: time control system systems state model optimal stability paper performance\n",
      "Topic 49: book windows music guide linux learn microsoft use new using\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def show_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx}: {' '.join(top_features)}\")\n",
    "\n",
    "show_top_words(lda, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0578de",
   "metadata": {},
   "source": [
    "# Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e708be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "lr= LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver=\"saga\",\n",
    "    n_jobs=-1\n",
    "    )\n",
    "\n",
    "# 3. Predykcja\n",
    "clf = OneVsRestClassifier(lr, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bin = clf.predict(X_test)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"models/classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95ab9e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = vectorizer.transform(test[\"Text\"])\n",
    "# lda_test_topic = lda.transform(X_test)\n",
    "\n",
    "\n",
    "# predict_binary = classifier.predict(lda_test_topic)\n",
    "# predict_labels = mlb.inverse_transform(predict_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24c54c",
   "metadata": {},
   "source": [
    "# Report\n",
    "- Example topics (keywords from LDA) DONE\n",
    "- PCA\n",
    "- precision/recall on validation data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
