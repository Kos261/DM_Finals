{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15bf63f5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Categorizing documents in **ACM Digital Library** example: \n",
    "\n",
    "(Main Class).(Subclass).(Subsubcategory)\n",
    "\n",
    "**H.3.5**\n",
    "\n",
    "* H. Information Systems\n",
    "    *  H.3 Information Storage and Retrieval\n",
    "        * H.3.5 Online Information Services\n",
    "\n",
    "**D.3.2**\n",
    "\n",
    "* D. Software\n",
    "    * D.3 Programming Languages\n",
    "        * D.3.2 Language Classifications\n",
    "  \n",
    "\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1dd6ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:16:59.198249Z",
     "start_time": "2025-06-01T14:16:56.074333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/konstanty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import os, pickle, joblib\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report, hamming_loss\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4c7c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique topics: 358\n",
      "First 10 example topics:  ['A.0', 'A.1', 'A.2', 'A.m', 'B.0', 'B.1', 'B.1.0', 'B.1.1', 'B.1.2', 'B.1.3']\n"
     ]
    }
   ],
   "source": [
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "test_path = os.path.join(notebook_dir, \"data\", \"DM2023_test_docs.tsv\")\n",
    "train_path = os.path.join(notebook_dir, \"data\", \"DM2023_training_docs_and_labels.tsv\")\n",
    "\n",
    "\n",
    "test = pd.read_csv(test_path, \n",
    "                    sep=\"\\t\", \n",
    "                    encoding=\"latin1\", \n",
    "                    header=None,\n",
    "                    names=[\"Textfile\", \"Text\", \"Topics\"])\n",
    "# test = test.drop_duplicates()\n",
    "                    \n",
    "                    \n",
    "train_full = pd.read_csv(train_path, \n",
    "                    sep=\"\\t\", \n",
    "                    encoding=\"latin1\", \n",
    "                    header=None,\n",
    "                    names=[\"Textfile\", \"Text\", \"Topics\"])\n",
    "\n",
    "\n",
    "def flatten_if_single(x):\n",
    "    \"\"\"Jeśli x jest listą długości 1 – zwróć jej pierwszy element.\"\"\"\n",
    "    if isinstance(x, list) and len(x) == 1:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "# Separating topics\n",
    "train_full[\"Topics\"] = (\n",
    "    train_full[\"Topics\"]\n",
    "    .apply(flatten_if_single)        \n",
    "    .str.split(r\"\\s*,\\s*\")         \n",
    ")\n",
    "\n",
    "# train[\"Topics\"] = train[\"Topics\"].str.split(\",\")\n",
    "\n",
    "unique_labels = set(label for sublist in train_full[\"Topics\"] for label in sublist)\n",
    "\n",
    "print(f\"Number of unique topics: {len(unique_labels)}\")\n",
    "print(\"First 10 example topics: \",sorted(list(unique_labels))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7af26a",
   "metadata": {},
   "source": [
    "# Train LDA, MLB (Or load) and topic distribution \n",
    "(Shape of mlb binary matrix should match the number of unique topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a3062a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (80000, 3)  Val: (20000, 3)\n",
      "Vectorizer loaded\n",
      "✓ MLB loaded\n",
      "Vectorizer vocab size: 467343\n",
      "y_train shape: (80000, 358)\n",
      "y_val   shape: (20000, 358)\n",
      "['A.0' 'A.1' 'A.2' 'A.m' 'B.0' 'B.1' 'B.1.0' 'B.1.1' 'B.1.2' 'B.1.3'\n",
      " 'B.1.4' 'B.1.5' 'B.1.m' 'B.2' 'B.2.0' 'B.2.1' 'B.2.2' 'B.2.3' 'B.2.4'\n",
      " 'B.2.m' 'B.3' 'B.3.0' 'B.3.1' 'B.3.2' 'B.3.3' 'B.3.4' 'B.3.m' 'B.4'\n",
      " 'B.4.0' 'B.4.1' 'B.4.2' 'B.4.3' 'B.4.4' 'B.4.5' 'B.4.m' 'B.5' 'B.5.0'\n",
      " 'B.5.1' 'B.5.2' 'B.5.3' 'B.5.m' 'B.6' 'B.6.0' 'B.6.1' 'B.6.2' 'B.6.3'\n",
      " 'B.6.m' 'B.7' 'B.7.0' 'B.7.1' 'B.7.2' 'B.7.3' 'B.7.m' 'B.8' 'B.8.0'\n",
      " 'B.8.1' 'B.8.2' 'B.8.m' 'B.m' 'C.0' 'C.1' 'C.1.0' 'C.1.1' 'C.1.2' 'C.1.3'\n",
      " 'C.1.4' 'C.1.m' 'C.2' 'C.2.0' 'C.2.1' 'C.2.2' 'C.2.3' 'C.2.4' 'C.2.5'\n",
      " 'C.2.6' 'C.2.m' 'C.3' 'C.4' 'C.5' 'C.5.0' 'C.5.1' 'C.5.2' 'C.5.3' 'C.5.4'\n",
      " 'C.5.5' 'C.5.m' 'C.m' 'D.0' 'D.1' 'D.1.0' 'D.1.1' 'D.1.2' 'D.1.3' 'D.1.4'\n",
      " 'D.1.5' 'D.1.6' 'D.1.7' 'D.1.m' 'D.2' 'D.2.0' 'D.2.1' 'D.2.10' 'D.2.11'\n",
      " 'D.2.12' 'D.2.13' 'D.2.2' 'D.2.3' 'D.2.4' 'D.2.5' 'D.2.6' 'D.2.7' 'D.2.8'\n",
      " 'D.2.9' 'D.2.m' 'D.3' 'D.3.0' 'D.3.1' 'D.3.2' 'D.3.3' 'D.3.4' 'D.3.m'\n",
      " 'D.4' 'D.4.0' 'D.4.1' 'D.4.2' 'D.4.3' 'D.4.4' 'D.4.5' 'D.4.6' 'D.4.7'\n",
      " 'D.4.8' 'D.4.9' 'D.4.m' 'D.m' 'E.0' 'E.1' 'E.2' 'E.3' 'E.4' 'E.5' 'E.m'\n",
      " 'F.0' 'F.1' 'F.1.0' 'F.1.1' 'F.1.2' 'F.1.3' 'F.1.m' 'F.2' 'F.2.0' 'F.2.1'\n",
      " 'F.2.2' 'F.2.3' 'F.2.m' 'F.3' 'F.3.0' 'F.3.1' 'F.3.2' 'F.3.3' 'F.3.m'\n",
      " 'F.4' 'F.4.0' 'F.4.1' 'F.4.2' 'F.4.3' 'F.4.m' 'F.m' 'G.0' 'G.1' 'G.1.0'\n",
      " 'G.1.1' 'G.1.10' 'G.1.2' 'G.1.3' 'G.1.4' 'G.1.5' 'G.1.6' 'G.1.7' 'G.1.8'\n",
      " 'G.1.9' 'G.1.m' 'G.2' 'G.2.0' 'G.2.1' 'G.2.2' 'G.2.3' 'G.2.m' 'G.3' 'G.4'\n",
      " 'G.m' 'H.0' 'H.1' 'H.1.0' 'H.1.1' 'H.1.2' 'H.1.m' 'H.2' 'H.2.0' 'H.2.1'\n",
      " 'H.2.2' 'H.2.3' 'H.2.4' 'H.2.5' 'H.2.6' 'H.2.7' 'H.2.8' 'H.2.m' 'H.3'\n",
      " 'H.3.0' 'H.3.1' 'H.3.2' 'H.3.3' 'H.3.4' 'H.3.5' 'H.3.6' 'H.3.7' 'H.3.m'\n",
      " 'H.4' 'H.4.0' 'H.4.1' 'H.4.2' 'H.4.3' 'H.4.m' 'H.5' 'H.5.0' 'H.5.1'\n",
      " 'H.5.2' 'H.5.3' 'H.5.4' 'H.5.5' 'H.5.m' 'H.m' 'I.0' 'I.1' 'I.1.0' 'I.1.1'\n",
      " 'I.1.2' 'I.1.3' 'I.1.4' 'I.1.m' 'I.2' 'I.2.0' 'I.2.1' 'I.2.10' 'I.2.11'\n",
      " 'I.2.2' 'I.2.3' 'I.2.4' 'I.2.5' 'I.2.6' 'I.2.7' 'I.2.8' 'I.2.9' 'I.2.m'\n",
      " 'I.3' 'I.3.0' 'I.3.1' 'I.3.2' 'I.3.3' 'I.3.4' 'I.3.5' 'I.3.6' 'I.3.7'\n",
      " 'I.3.8' 'I.3.m' 'I.4' 'I.4.0' 'I.4.1' 'I.4.10' 'I.4.2' 'I.4.3' 'I.4.4'\n",
      " 'I.4.5' 'I.4.6' 'I.4.7' 'I.4.8' 'I.4.9' 'I.4.m' 'I.5' 'I.5.0' 'I.5.1'\n",
      " 'I.5.2' 'I.5.3' 'I.5.4' 'I.5.5' 'I.5.m' 'I.6' 'I.6.0' 'I.6.1' 'I.6.2'\n",
      " 'I.6.3' 'I.6.4' 'I.6.5' 'I.6.6' 'I.6.7' 'I.6.8' 'I.6.m' 'I.7' 'I.7.0'\n",
      " 'I.7.1' 'I.7.2' 'I.7.3' 'I.7.4' 'I.7.5' 'I.7.m' 'I.m' 'J' 'J.0' 'J.1'\n",
      " 'J.2' 'J.3' 'J.4' 'J.5' 'J.6' 'J.7' 'J.m' 'K.0' 'K.1' 'K.2' 'K.3' 'K.3.0'\n",
      " 'K.3.1' 'K.3.2' 'K.3.m' 'K.4' 'K.4.0' 'K.4.1' 'K.4.2' 'K.4.3' 'K.4.4'\n",
      " 'K.4.m' 'K.5' 'K.5.0' 'K.5.1' 'K.5.2' 'K.5.m' 'K.6' 'K.6.0' 'K.6.1'\n",
      " 'K.6.2' 'K.6.3' 'K.6.4' 'K.6.5' 'K.6.m' 'K.7' 'K.7.0' 'K.7.1' 'K.7.2'\n",
      " 'K.7.3' 'K.7.4' 'K.7.m' 'K.8' 'K.8.0' 'K.8.1' 'K.8.2' 'K.8.3' 'K.8.m'\n",
      " 'K.m']\n"
     ]
    }
   ],
   "source": [
    "MODELDIR = Path(\"models\")\n",
    "MODELDIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def load_or_train_mlb(train_labels, *, path=Path(\"models/mlb_model.pkl\"),\n",
    "                      all_labels=None):\n",
    "    if path.exists():\n",
    "        print(\"✓ MLB loaded\")\n",
    "        return joblib.load(path)\n",
    "    print(\"… training MultiLabelBinarizer\")\n",
    "    if all_labels is None:\n",
    "        all_labels = sorted({lbl for sub in train_labels for lbl in sub})\n",
    "    mlb = MultiLabelBinarizer(classes=all_labels)\n",
    "    mlb.fit(train_labels)\n",
    "    joblib.dump(mlb, path)\n",
    "    return mlb\n",
    "\n",
    "val   = train_full.iloc[80_000:].reset_index(drop=True)\n",
    "train = train_full.iloc[:80_000].reset_index(drop=True)\n",
    "print(\"Train:\", train.shape, \" Val:\", val.shape)\n",
    "\n",
    "vec_path = Path(\"models/vectorizer.pkl\")\n",
    "if vec_path.exists():\n",
    "    print(\"Vectorizer loaded\")\n",
    "    vectorizer = joblib.load(vec_path)\n",
    "else:\n",
    "    print(\"training Vectorizer...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        max_df=0.9,\n",
    "        min_df=3,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "        max_features=100_000\n",
    "    )\n",
    "    vectorizer.fit(train[\"Text\"])\n",
    "    joblib.dump(vectorizer, \"models/vectorizer.pkl\")\n",
    "\n",
    "\n",
    "X_train_topics = vectorizer.transform(train[\"Text\"])\n",
    "X_val_topics   = vectorizer.transform(val[\"Text\"])\n",
    "X_test_topics  = vectorizer.transform(test[\"Text\"])\n",
    "\n",
    "all_topics = sorted({lbl for sub in train_full[\"Topics\"] for lbl in sub})\n",
    "\n",
    "mlb = load_or_train_mlb(train[\"Topics\"], all_labels=all_topics)\n",
    "\n",
    "y_train = mlb.transform(train[\"Topics\"])\n",
    "y_val   = mlb.transform(val[\"Topics\"])\n",
    "\n",
    "print(\"Vectorizer vocab size:\", len(vectorizer.get_feature_names_out()))\n",
    "# print(\"LDA topics:\", lda.n_components)\n",
    "print(\"y_train shape:\", y_train.shape)        # (80000, 358)\n",
    "print(\"y_val   shape:\", y_val.shape)          # (20000, 358)\n",
    "print(mlb.classes_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4ba14",
   "metadata": {},
   "source": [
    "# Check 5 topics distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49072bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_ids = train[\"Textfile\"].iloc[:5].values\n",
    "# topic_distributions = X_train_topics[:5]\n",
    "\n",
    "# topics_df = pd.DataFrame(np.round(topic_distributions, 3),\n",
    "#                                    columns=[f\"Topic {i}\" for i in range(lda.n_components)],\n",
    "#                                    index=file_ids)\n",
    "# topics_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992e7a8",
   "metadata": {},
   "source": [
    "# Let's see words assigned to different topics with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea0754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# def show_top_words(model, feature_names, n_top_words=10):\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "#         top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "#         print(f\"Topic {topic_idx}: {' '.join(top_features)}\")\n",
    "\n",
    "# show_top_words(lda, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0578de",
   "metadata": {},
   "source": [
    "# Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83e708be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to train classifier first...\n",
      "✅ Najlepszy próg = -0.500, F1 = 0.4332\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"models/classifier.pkl\"):\n",
    "    print(\"Found classifier model!\")\n",
    "\n",
    "    with open(\"models/classifier.pkl\", \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print(\"We need to train classifier first...\")\n",
    "\n",
    "    lr = LinearSVC(C=1.0, dual=False)\n",
    "\n",
    "    # 3. Predykcja\n",
    "    clf = OneVsRestClassifier(lr, n_jobs=-1)\n",
    "    clf.fit(X_train_topics, y_train)\n",
    "\n",
    "\n",
    "    margins = clf.decision_function(X_val_topics)\n",
    "\n",
    "    # Przetestuj wiele progów\n",
    "    best_f1, best_thr = -1, 0\n",
    "    for thr in np.linspace(-0.5, 1.0, 30):\n",
    "        y_pred_bin = (margins > thr).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred_bin, average=\"samples\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "\n",
    "    print(f\"✅ Najlepszy próg = {best_thr:.3f}, F1 = {best_f1:.4f}\")\n",
    "\n",
    "    with open(\"models/classifier.pkl\", \"wb\") as f:\n",
    "        pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a1a5b",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dd58f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation...\n",
      "[['F.2.2', 'H.2.8', 'I.2.6', 'I.5.2'], ['C.2.1', 'C.2.2', 'C.2.6', 'C.4', 'F.2.2'], ['F.1.1', 'F.1.2', 'F.4.3', 'I.2.8'], ['F.2.2', 'G.1.10', 'G.1.8', 'J.2'], ['F.1.2', 'F.2.2', 'G.3'], ['D.2.4', 'D.2.5'], [], ['F.1.2', 'G.1.0'], ['C.2.2', 'H.4.3', 'H.5.1', 'H.5.2'], ['H.4.2'], ['I.4.6', 'I.4.7', 'I.4.8', 'I.5.2'], ['C.2.0', 'C.2.3', 'C.2.4', 'D.4.6', 'K.6.5'], ['D.2.2', 'D.3.2', 'H.5.4'], ['F.1.1', 'I.2.3', 'I.2.6', 'I.5.1', 'I.6.4', 'I.6.5', 'J.2', 'J.7'], ['K.4.1', 'K.4.3', 'K.7.4'], ['K.3.2'], ['G.1.6', 'I.2.8', 'I.6.8', 'J.1'], ['D.2.5', 'D.2.7', 'F.2.2', 'I.2.8'], ['B.6.1', 'B.6.3', 'B.8.1', 'J.2', 'J.6'], ['C.2.2', 'K.6.5'], ['C.2.4', 'C.4', 'D.4.3', 'D.4.5', 'D.4.7', 'D.4.8'], ['F.2.2', 'G.1.6', 'G.2.2', 'I.2.8'], ['C.2.0', 'C.2.1', 'D.3.2', 'K.6.1', 'K.6.5'], ['H.1.2', 'H.5.2', 'K.6.1'], ['C.2.2', 'C.4', 'D.2.1'], ['G.1.2', 'I.3.5'], ['C.2.1', 'C.2.3', 'C.2.5', 'C.4'], ['C.2.1', 'C.2.4'], ['E.4', 'G.3'], ['D.2.11', 'D.2.6', 'K.6.3'], ['C.2.1', 'C.2.3', 'C.4', 'G.3'], ['B.7.2', 'B.8.1', 'B.8.2'], ['I.2.3', 'I.2.6', 'I.2.8', 'I.5.1', 'J.1', 'J.6'], ['J.1', 'K.3.1'], ['H.5.2', 'H.5.4', 'K.4.2'], ['D.2.8', 'K.6.1', 'K.6.3'], ['K.6.4'], ['H.3.5', 'K.6.1'], ['D.1.5', 'D.2.2', 'D.3.2', 'H.5.1', 'I.3.7', 'K.3.1'], ['B.7.1', 'J.2'], ['D.2.1', 'H.5.2', 'I.6.5'], ['F.2.2', 'G.1.2', 'G.2.2'], ['H.1.2', 'H.3.3', 'H.3.4', 'H.3.5', 'H.3.7', 'I.2.0', 'I.5.2'], ['H.2.8', 'H.5.2'], ['D.3.4', 'F.1.3', 'F.4.2', 'F.4.3', 'I.2.7'], ['F.2.0', 'F.2.2', 'K.8.0'], ['I.6.5', 'J.3'], ['C.2.0', 'D.4.6', 'K.6.5'], ['F.1.3', 'I.2.11'], ['D.4.0', 'K.3.2', 'K.6.5'], ['F.2.2', 'I.2.10', 'I.4.6', 'I.4.7', 'I.4.8'], ['C.2.1', 'C.2.3', 'F.2.2', 'G.1.6'], ['I.2.10', 'I.4.0', 'I.4.7', 'I.4.8', 'I.4.9', 'I.5.4'], ['G.1.0', 'G.1.6', 'I.2.3', 'I.2.8'], ['B.6.3', 'D.2.4'], ['H.3.3'], ['H.3.3', 'H.3.4'], ['H.1.2', 'H.5.2', 'H.5.3', 'J.1'], ['C.2.4', 'I.2.11'], [], ['D.4.2', 'D.4.3', 'H.3.3'], ['H.3.5', 'K.4.4', 'K.6.1'], [], ['H.1.2', 'I.4.7', 'I.4.8'], ['I.3.5', 'I.3.7'], ['H.4.2'], ['D.2.4', 'I.2.8'], ['C.4'], ['C.2.1', 'C.2.2', 'C.2.3', 'C.4'], ['J.1', 'K.4.3', 'K.4.4'], ['C.2.0', 'C.2.1', 'C.2.2', 'C.2.4', 'C.4'], ['I.2.10', 'I.2.9', 'I.4.8', 'I.5.2', 'I.5.4'], ['H.1.2', 'I.6.5', 'J.1', 'K.3.1', 'K.6.1'], ['H.3.4', 'J.1', 'K.4.4'], ['H.3.5', 'K.4.3'], ['H.5.5', 'I.5.4', 'J.5'], ['G.1.2', 'I.2.8', 'I.5.4'], ['I.2.6', 'I.4.7'], ['B.8.1', 'C.3', 'C.4', 'D.4.5', 'D.4.7'], ['I.3.5'], ['H.3.3', 'H.4.3', 'H.5.2', 'I.5.2'], ['H.3.3', 'H.3.4', 'K.4.1'], ['H.4.2', 'I.2.8', 'J.1', 'K.6.1'], ['G.3', 'I.5.1', 'I.5.2', 'I.6.4'], ['H.5.2', 'K.6.5'], ['I.3.5', 'I.3.7'], ['F.1.3', 'F.2.2', 'G.2.1', 'G.2.2'], ['J.1', 'K.3.1', 'K.3.2'], ['C.2.0', 'H.3.4', 'K.6.5'], ['H.1.2', 'H.5.2', 'K.8.0'], ['G.1.3', 'G.3'], ['F.2.2', 'F.4.1'], ['C.2.1', 'C.2.3', 'H.3.1', 'I.2.6'], ['H.4.2', 'I.6.5', 'K.6.1'], ['H.5.3'], ['D.2.2', 'D.2.4', 'F.3.1', 'F.4.1', 'I.6.4'], ['C.3', 'D.2.13', 'D.2.2', 'D.2.7', 'D.3.2', 'D.4.7', 'H.3.7', 'I.2.2'], ['G.2.2', 'H.2.4', 'I.2.8', 'I.7.2'], ['C.4'], ['I.6.5', 'J.2', 'J.7']]\n",
      "micro-F1 : 0.4530077237238245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstanty/Projects/UW/UWvenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 : 0.21185145999027402\n",
      "sample-F1 : 0.43321898878898873\n",
      "Hamming  : 0.00910963687150838\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation...\")\n",
    "# y_pred_bin = clf.predict(X_val_topics)\n",
    "y_pred_bin = (clf.decision_function(X_val_topics) > best_thr).astype(int)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "predicted_topics_list = [list(labels) for labels in y_pred_labels]\n",
    "print(predicted_topics_list[:100])\n",
    "\n",
    "val[\"PredictedTopics\"] = predicted_topics_list\n",
    "\n",
    "y_val_true_bin = mlb.transform(val[\"Topics\"])\n",
    "y_val_pred_bin = mlb.transform(val[\"PredictedTopics\"])\n",
    "\n",
    "\n",
    "print(\"micro-F1 :\", f1_score(y_val_true_bin, y_val_pred_bin, average=\"micro\"))\n",
    "print(\"macro-F1 :\", f1_score(y_val_true_bin, y_val_pred_bin, average=\"macro\"))\n",
    "print(\"sample-F1 :\", f1_score(y_val_true_bin, y_val_pred_bin, average=\"samples\"))\n",
    "print(\"Hamming  :\", hamming_loss(y_val_true_bin, y_val_pred_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a235947",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b07d3c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction...\n",
      "['K.6.1']\n",
      "['H.1.2', 'H.5.1', 'H.5.2']\n",
      "['G.1.0', 'G.1.7']\n",
      "['C.2.1', 'C.3']\n",
      "['K.6.1']\n",
      "['I.2.7']\n",
      "['K.6.1']\n",
      "['I.2.3']\n",
      "['C.1.2', 'C.4', 'G.2.2']\n",
      "['G.1.1', 'I.3.5']\n",
      "['I.2.11', 'I.2.4']\n",
      "['I.6.5']\n",
      "['C.2.1']\n",
      "['K.6.1']\n",
      "['F.2.2', 'I.2.8']\n",
      "['B.7.1', 'B.7.2', 'B.8.2']\n",
      "['F.2.2', 'J.3']\n",
      "['H.3.5']\n",
      "['I.3.7']\n",
      "['H.5.1']\n",
      "['B.7.1']\n",
      "['K.3.2']\n",
      "['H.3.5']\n",
      "['E.4']\n",
      "['J.3']\n",
      "['H.5.2']\n",
      "['D.1.3']\n",
      "['K.3.2']\n",
      "['I.4.8']\n",
      "       Textfile       Predicted Topics\n",
      "0    963168.txt                [K.6.1]\n",
      "1   1811004.txt                     []\n",
      "2    192631.txt  [H.1.2, H.5.1, H.5.2]\n",
      "3   1183872.txt                     []\n",
      "4   1280491.txt         [G.1.0, G.1.7]\n",
      "5   1059284.txt           [C.2.1, C.3]\n",
      "6   1133457.txt                [K.6.1]\n",
      "7   1140350.txt                     []\n",
      "8    100973.txt                [I.2.7]\n",
      "9   1147150.txt                     []\n",
      "10   598535.txt                     []\n",
      "11  1318072.txt                [K.6.1]\n",
      "12  1222053.txt                [I.2.3]\n",
      "13   110442.txt    [C.1.2, C.4, G.2.2]\n",
      "14  1044226.txt         [G.1.1, I.3.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prediction...\")\n",
    "y_pred_bin = clf.predict(X_test_topics)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "# predicted_topics_str = [\",\".join(labels) if labels else \"-\" for labels in y_pred_labels]\n",
    "predicted_topics_list = [list(labels) for labels in y_pred_labels]\n",
    "for topics in predicted_topics_list[:50]:\n",
    "    if topics != []:\n",
    "        print(topics)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Textfile\": test[\"Textfile\"].values,\n",
    "    \"Predicted Topics\": predicted_topics_list\n",
    "})\n",
    "\n",
    "\n",
    "# Making sure this has the same order\n",
    "order = test[\"Textfile\"]\n",
    "\n",
    "\n",
    "results_sorted = (\n",
    "    results.set_index(\"Textfile\")   # <- klucz do dopasowania\n",
    "           .loc[order]              # <- reindex wg referencyjnej kolejności\n",
    "           .reset_index()           # <- wróć do zwykłej kolumny\n",
    ")\n",
    "\n",
    "# 3. (opcjonalnie) nadpisz `results`\n",
    "results = results_sorted\n",
    "\n",
    "print(results.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
