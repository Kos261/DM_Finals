{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86a3b55",
   "metadata": {},
   "source": [
    "# ACM Abstract Topic Classifier\n",
    "MiniLM embedding + LogisticRegression (One‑vs‑Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba95bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstanty/Projects/UW/UWvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/konstanty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & environment ---\n",
    "import os, pickle, joblib, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score, hamming_loss)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "MODELDIR = Path('models')\n",
    "MODELDIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee551d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def flatten_if_single(x):\n",
    "    \"\"\"If x is a list of length 1, return its first element.\"\"\"\n",
    "    if isinstance(x, list) and len(x) == 1:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "def load_or_train_mlb(train_labels, path=Path('models/mlb_model.pkl'), all_labels=None):\n",
    "    if path.exists():\n",
    "        print('✓ MLB loaded')\n",
    "        return joblib.load(path)\n",
    "    print('… training MultiLabelBinarizer')\n",
    "    if all_labels is None:\n",
    "        all_labels = sorted({lbl for sub in train_labels for lbl in sub})\n",
    "    mlb = MultiLabelBinarizer(classes=all_labels)\n",
    "    mlb.fit(train_labels)\n",
    "    joblib.dump(mlb, path)\n",
    "    return mlb\n",
    "\n",
    "def notify():\n",
    "    try:\n",
    "        subprocess.run(['play', '-nq', '-t', 'alsa', 'synth', '0.3', 'sine', '1000'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdad195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "notebook_dir = os.getcwd()\n",
    "test_path  = os.path.join(notebook_dir, 'data', 'DM2023_test_docs.tsv')\n",
    "train_path = os.path.join(notebook_dir, 'data', 'DM2023_training_docs_and_labels.tsv')\n",
    "\n",
    "test = pd.read_csv(test_path,  sep='\\t', encoding='latin1',\n",
    "                   header=None, names=['Textfile','Text','Topics'])\n",
    "\n",
    "train_full = pd.read_csv(train_path, sep='\\t', encoding='latin1',\n",
    "                         header=None, names=['Textfile','Text','Topics'])\n",
    "\n",
    "# Split topics string into list\n",
    "train_full['Topics'] = (train_full['Topics']\n",
    "                        .apply(flatten_if_single)\n",
    "                        .str.split(r'\\s*,\\s*'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabc8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (80000, 3)  Val: (20000, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- Train / Validation split ---\n",
    "split = int(train_full.shape[0] * 0.8)\n",
    "val   = train_full.iloc[split:].reset_index(drop=True)\n",
    "train = train_full.iloc[:split].reset_index(drop=True)\n",
    "print('Train:', train.shape, ' Val:', val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92028b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t model loaded\n"
     ]
    }
   ],
   "source": [
    "# --- SentenceTransformer Embeddings ---\n",
    "bert_path = Path('models/minilm_model.pkl')  # change filename if you switch model\n",
    "\n",
    "if bert_path.exists():\n",
    "    print('\\n\\t model loaded')\n",
    "    model = joblib.load(bert_path)\n",
    "else:\n",
    "    print('\\n\\t… loading MiniLM')\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "    joblib.dump(model, bert_path)\n",
    "\n",
    "embed_paths = [Path('models/X_train_embed.npy'),\n",
    "               Path('models/X_val_embed.npy'),\n",
    "               Path('models/X_test_embed.npy')]\n",
    "\n",
    "if all(p.exists() for p in embed_paths):\n",
    "    X_train_embed = np.load(embed_paths[0])\n",
    "    X_val_embed   = np.load(embed_paths[1])\n",
    "    X_test_embed  = np.load(embed_paths[2])\n",
    "else:\n",
    "    X_train_embed = model.encode(train['Text'].tolist(), batch_size=32, show_progress_bar=True)\n",
    "    X_val_embed   = model.encode(val['Text'].tolist(),   batch_size=32, show_progress_bar=True)\n",
    "    X_test_embed  = model.encode(test['Text'].tolist(),  batch_size=32, show_progress_bar=True)\n",
    "\n",
    "    np.save(embed_paths[0], X_train_embed)\n",
    "    np.save(embed_paths[1], X_val_embed)\n",
    "    np.save(embed_paths[2], X_test_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a22fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLB loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstanty/Projects/UW/UWvenv/lib/python3.12/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Label binarization ---\n",
    "all_topics = sorted({lbl for sub in train_full['Topics'] for lbl in sub})\n",
    "mlb = load_or_train_mlb(train['Topics'], all_labels=all_topics)\n",
    "\n",
    "y_train = mlb.transform(train['Topics'])\n",
    "y_val   = mlb.transform(val['Topics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b035b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWe need to train classifier first...\n"
     ]
    }
   ],
   "source": [
    "# --- Train or load classifier ---\n",
    "clf_path = Path('models/classifier.pkl')\n",
    "\n",
    "if clf_path.exists():\n",
    "    print('\\n\\tFound classifier model!')\n",
    "    with open(clf_path, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "else:\n",
    "    print('\\n\\tWe need to train classifier first...')\n",
    "    # base = LogisticRegression(penalty='l2', \n",
    "    #                           C=1.0, \n",
    "    #                           dual=False, \n",
    "    #                           class_weight='balanced',\n",
    "    #                           solver='saga', \n",
    "    #                           max_iter=300,\n",
    "    #                           random_state=42)\n",
    "    base = LinearSVC(C=1.0, dual=False)     # dual=False szybsze dla n_samples > n_features\n",
    "    clf = OneVsRestClassifier(base, n_jobs=-1)\n",
    "    clf.fit(X_train_embed, y_train)\n",
    "    joblib.dump(clf, clf_path)\n",
    "    notify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf25886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== SAMPLE-BASED METRICS ===\n",
      "Precision (samples): 0.5559144047619047\n",
      "Recall    (samples): 0.3821262518037518\n",
      "F1        (samples): 0.4141946558996559\n",
      "Subset accuracy   : 0.06345\n",
      "Hamming loss      : 0.0073371508379888265\n"
     ]
    }
   ],
   "source": [
    "# --- Validation ---\n",
    "# y_pred_bin = clf.predict(X_val_embed)  # default threshold\n",
    "# y_pred_bin = (clf.decision_function(X_val_embed) > -0.3).astype(int)\n",
    "\n",
    "\n",
    "scores = clf.decision_function(X_val_embed)\n",
    "y_pred_bin = np.zeros_like(scores, dtype=int)\n",
    "\n",
    "for i, row in enumerate(scores):\n",
    "    top_idx = row.argmax()\n",
    "    y_pred_bin[i, top_idx] = 1                 # zawsze co najmniej 1 etykieta\n",
    "    y_pred_bin[i, row > -0.3] = 1          # + inne, które przekroczą próg\n",
    "\n",
    "\n",
    "val['PredictedTopics'] = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "y_val_true_bin = mlb.transform(val['Topics'])\n",
    "\n",
    "print('\\n\\n=== SAMPLE-BASED METRICS ===')\n",
    "print('Precision (samples):', precision_score(y_val_true_bin, y_pred_bin,\n",
    "                                             average='samples', zero_division=0))\n",
    "print('Recall    (samples):', recall_score(y_val_true_bin, y_pred_bin, average='samples'))\n",
    "print('F1        (samples):', f1_score(y_val_true_bin, y_pred_bin,\n",
    "                                       average='samples', zero_division=0))\n",
    "print('Subset accuracy   :', accuracy_score(y_val_true_bin, y_pred_bin))\n",
    "print('Hamming loss      :', hamming_loss(y_val_true_bin, y_pred_bin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915b4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict on test & save submission ---\n",
    "# y_test_bin = clf.predict(X_test_embed)\n",
    "# \n",
    "\n",
    "scores = clf.decision_function(X_test_embed)\n",
    "y_test_bin = np.zeros_like(scores, dtype=int)\n",
    "\n",
    "for i, row in enumerate(scores):\n",
    "    top_idx = row.argmax()\n",
    "    y_test_bin[i, top_idx] = 1                 # zawsze co najmniej 1 etykieta\n",
    "    y_test_bin[i, row > -0.3] = 1          # + inne, które przekroczą próg\n",
    "\n",
    "label_lists = mlb.inverse_transform(y_test_bin)\n",
    "\n",
    "submission = test\n",
    "submission[\"Topics\"] = label_lists #.str.join(\",\")\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "notify()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
