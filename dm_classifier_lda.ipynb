{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86a3b55",
   "metadata": {},
   "source": [
    "# ACM Abstract Topic Classifier\n",
    "MiniLM embedding + LogisticRegression (One‑vs‑Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba95bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/konstanty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & environment ---\n",
    "import os, pickle, joblib, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score, hamming_loss)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee551d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def flatten_if_single(x):\n",
    "    \"\"\"If x is a list of length 1, return its first element.\"\"\"\n",
    "    if isinstance(x, list) and len(x) == 1:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "def load_or_train_mlb(train_labels, path=Path('models/LDA/mlb_model.pkl'), all_labels=None):\n",
    "    if path.exists():\n",
    "        print('✓ MLB loaded')\n",
    "        return joblib.load(path)\n",
    "    print('… training MultiLabelBinarizer')\n",
    "    if all_labels is None:\n",
    "        all_labels = sorted({lbl for sub in train_labels for lbl in sub})\n",
    "    mlb = MultiLabelBinarizer(classes=all_labels)\n",
    "    mlb.fit(train_labels)\n",
    "    joblib.dump(mlb, path)\n",
    "    return mlb\n",
    "\n",
    "def notify():\n",
    "    try:\n",
    "        subprocess.run(['play', '-nq', '-t', 'alsa', 'synth', '0.3', 'sine', '1000'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cdad195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "notebook_dir = os.getcwd()\n",
    "test_path  = os.path.join(notebook_dir, 'data', 'DM2023_test_docs.tsv')\n",
    "train_path = os.path.join(notebook_dir, 'data', 'DM2023_training_docs_and_labels.tsv')\n",
    "\n",
    "test = pd.read_csv(test_path,  sep='\\t', encoding='latin1',\n",
    "                   header=None, names=['Textfile','Text','Topics'])\n",
    "\n",
    "train_full = pd.read_csv(train_path, sep='\\t', encoding='latin1',\n",
    "                         header=None, names=['Textfile','Text','Topics'])\n",
    "\n",
    "# Split topics string into list\n",
    "train_full['Topics'] = (train_full['Topics']\n",
    "                        .apply(flatten_if_single)\n",
    "                        .str.split(r'\\s*,\\s*'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cabc8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (80000, 3)  Val: (20000, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- Train / Validation split ---\n",
    "split = int(train_full.shape[0] * 0.8)\n",
    "val   = train_full.iloc[split:].reset_index(drop=True)\n",
    "train = train_full.iloc[:split].reset_index(drop=True)\n",
    "print('Train:', train.shape, ' Val:', val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92028b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vectorizer ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Teksty\n",
    "train_texts = train[\"Text\"].tolist()\n",
    "val_texts   = val[\"Text\"].tolist()\n",
    "test_texts  = test[\"Text\"].tolist()\n",
    "\n",
    "# Wektoryzacja\n",
    "\n",
    "vec_path = Path('models/LDA/vectorizer.pkl')\n",
    "\n",
    "if vec_path.exists():\n",
    "    print('\\n\\tFound vectorizer model!')\n",
    "    with open(vec_path, 'rb') as f:\n",
    "        vectorizer = joblib.load(f)\n",
    "    X_train_bow = vectorizer.transform(train_texts)\n",
    "\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words=stop_words)\n",
    "    X_train_bow = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "\n",
    "X_val_bow   = vectorizer.transform(val_texts)\n",
    "X_test_bow  = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc820299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWe need to train LDA first...\n"
     ]
    }
   ],
   "source": [
    "# --- LDA ---\n",
    "vec_path = Path('models/LDA/lda.pkl')\n",
    "\n",
    "if vec_path.exists():\n",
    "    print('\\n\\tFound LDA model!')\n",
    "    with open(vec_path, 'rb') as f:\n",
    "        lda = joblib.load(f)\n",
    "else:\n",
    "    print(\"\\n\\tWe need to train LDA first...\")\n",
    "    lda = LatentDirichletAllocation(n_components=20, \n",
    "                                    max_iter=10, \n",
    "                                    learning_method='batch', \n",
    "                                    random_state=42)\n",
    "    \n",
    "X_train_lda = lda.fit_transform(X_train_bow)\n",
    "X_val_lda   = lda.transform(X_val_bow)\n",
    "X_test_lda  = lda.transform(X_test_bow)\n",
    "\n",
    "lda_paths = [Path('models/LDA/X_train_lda.npy'),\n",
    "               Path('models/LDA/X_val_lda.npy'),\n",
    "               Path('models/LDA/X_test_lda.npy')]\n",
    "\n",
    "if all(p.exists() for p in lda_paths):\n",
    "    X_train_lda = np.load(lda_paths[0])\n",
    "    X_val_lda   = np.load(lda_paths[1])\n",
    "    X_test_lda  = np.load(lda_paths[2])\n",
    "else:\n",
    "    X_train_lda = lda.fit_transform(X_train_bow)\n",
    "    X_val_lda   = lda.transform(X_val_bow)\n",
    "    X_test_lda  = lda.transform(X_test_bow)\n",
    "\n",
    "    np.save(lda_paths[0], X_train_lda)\n",
    "    np.save(lda_paths[1], X_val_lda)\n",
    "    np.save(lda_paths[2], X_test_lda)\n",
    "notify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddd7d9",
   "metadata": {},
   "source": [
    "# Showing to 10 words in topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca59d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top słowa dla każdego tematu:\n",
      "\n",
      "Temat 1: memory, performance, parallel, code, program, system, time, data, execution, programs\n",
      "Temat 2: web, search, attacks, pages, user, users, information, detection, attack, page\n",
      "Temat 3: classification, data, learning, clustering, based, algorithm, method, neural, recognition, feature\n",
      "Temat 4: data, query, xml, database, queries, security, mining, information, databases, ontology\n",
      "Temat 5: image, images, 3d, method, motion, based, shape, surface, algorithm, objects\n",
      "Temat 6: problem, algorithm, graph, graphs, problems, algorithms, linear, polynomial, number, time\n",
      "Temat 7: game, games, virtual, interaction, user, speech, rfid, haptic, interface, mobile\n",
      "Temat 8: model, models, time, distribution, stochastic, estimation, distributions, probability, analysis, random\n",
      "Temat 9: signal, protein, data, gene, noise, frequency, method, channel, signals, algorithm\n",
      "Temat 10: robot, control, agents, agent, robots, controller, system, fuzzy, systems, learning\n",
      "Temat 11: video, multimedia, mpeg, videos, audio, tv, content, media, streaming, frames\n",
      "Temat 12: network, networks, wireless, routing, sensor, protocol, mobile, traffic, nodes, performance\n",
      "Temat 13: peer, grid, distributed, p2p, storage, file, data, system, server, performance\n",
      "Temat 14: web, services, service, user, system, software, systems, design, based, applications\n",
      "Temat 15: software, information, research, development, design, learning, systems, technology, study, computer\n",
      "Temat 16: power, design, test, circuit, circuits, fault, chip, algorithm, voltage, based\n",
      "Temat 17: apl, wiley, conference, workshop, 2007, articles, 2008, 2006, papers, ltd\n",
      "Temat 18: numerical, method, equations, equation, finite, element, boundary, model, flow, order\n",
      "Temat 19: language, logic, model, formal, semantics, specification, systems, reasoning, languages, object\n",
      "Temat 20: students, course, book, computer, programming, science, teaching, courses, learn, student\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nTop słowa dla każdego tematu:\\n\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_features_ind = topic.argsort()[::-1][:n_top_words]\n",
    "    top_words = [feature_names[i] for i in top_features_ind]\n",
    "    print(f\"Temat {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a22fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "… training MultiLabelBinarizer\n"
     ]
    }
   ],
   "source": [
    "# --- Label binarization ---\n",
    "all_topics = sorted({lbl for sub in train_full['Topics'] for lbl in sub})\n",
    "mlb = load_or_train_mlb(train['Topics'], all_labels=all_topics)\n",
    "\n",
    "y_train = mlb.transform(train['Topics'])\n",
    "y_val   = mlb.transform(val['Topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5cfae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWe need to train classifier first...\n"
     ]
    }
   ],
   "source": [
    "# --- Train or load classifier ---\n",
    "clf_path = Path('models/LDA/classifier_lda.pkl')\n",
    "\n",
    "if clf_path.exists():\n",
    "    print('\\n\\tFound classifier model!')\n",
    "    with open(clf_path, 'rb') as f:\n",
    "        clf = joblib.load(f)\n",
    "else:\n",
    "    print('\\n\\tWe need to train classifier first...')\n",
    "   \n",
    "    base = LinearSVC(C=1.0, dual=False)     # dual=False szybsze dla n_samples > n_features\n",
    "    clf = OneVsRestClassifier(base, n_jobs=-1)\n",
    "    clf.fit(X_train_lda, y_train)\n",
    "    joblib.dump(clf, clf_path)\n",
    "    notify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf25886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== SAMPLE-BASED METRICS ===\n",
      "Precision (samples): 0.3018125\n",
      "Recall    (samples): 0.139052601010101\n",
      "F1        (samples): 0.17555406204906204\n",
      "Subset accuracy   : 0.0308\n",
      "Hamming loss      : 0.008949301675977654\n"
     ]
    }
   ],
   "source": [
    "# --- Validation ---\n",
    "\n",
    "scores = clf.decision_function(X_val_lda)\n",
    "y_pred_bin = np.zeros_like(scores, dtype=int)\n",
    "\n",
    "for i, row in enumerate(scores):\n",
    "    top_idx = row.argmax()\n",
    "    y_pred_bin[i, top_idx] = 1                 # zawsze co najmniej 1 etykieta\n",
    "    y_pred_bin[i, row > -0.3] = 1          # + inne, które przekroczą próg\n",
    "\n",
    "\n",
    "val['PredictedTopics'] = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "y_val_true_bin = mlb.transform(val['Topics'])\n",
    "\n",
    "print('\\n\\n=== SAMPLE-BASED METRICS ===')\n",
    "print('Precision (samples):', precision_score(y_val_true_bin, y_pred_bin,\n",
    "                                             average='samples', zero_division=0))\n",
    "print('Recall    (samples):', recall_score(y_val_true_bin, y_pred_bin, average='samples'))\n",
    "print('F1        (samples):', f1_score(y_val_true_bin, y_pred_bin,\n",
    "                                       average='samples', zero_division=0))\n",
    "print('Subset accuracy   :', accuracy_score(y_val_true_bin, y_pred_bin))\n",
    "print('Hamming loss      :', hamming_loss(y_val_true_bin, y_pred_bin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915b4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict on test & save submission ---\n",
    "\n",
    "scores = clf.decision_function(X_test_lda)\n",
    "y_test_bin = np.zeros_like(scores, dtype=int)\n",
    "\n",
    "for i, row in enumerate(scores):\n",
    "    top_idx = row.argmax()\n",
    "    y_test_bin[i, top_idx] = 1                 # zawsze co najmniej 1 etykieta\n",
    "    y_test_bin[i, row > -0.3] = 1              # + inne, które przekroczą próg\n",
    "\n",
    "label_lists = mlb.inverse_transform(y_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1be9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "submission = test\n",
    "submission[\"Topics\"] = [\",\".join(labels) for labels in label_lists]\n",
    "\n",
    "with open(\"output/submission_lda.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for labels in label_lists:\n",
    "        f.write(\",\".join(labels) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
